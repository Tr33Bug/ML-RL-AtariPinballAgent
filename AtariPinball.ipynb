{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0CXP1j7ZE-y"
      },
      "source": [
        "# Video Pinball"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfohG30UZE-1"
      },
      "source": [
        "This project aims to teach a reinforcement learning agent to play the game [Atari Video Pinball](https://gymnasium.farama.org/environments/atari/video_pinball/).  \n",
        "For this purpose, the Deep Q-Learning approach is followed using a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oha95PZYZE-2"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uto_yJs4ZE-3"
      },
      "source": [
        "General information:\n",
        "\n",
        "|                   |                                   |\n",
        "| ----------------- | --------------------------------- |\n",
        "| Action Space      | Discrete(18)                      |\n",
        "| Observation Space | (210, 160, 3)                     |\n",
        "| Observation High  | 255                               |\n",
        "| Observation Low   | 0                                 |\n",
        "| Import            | `gym.make(\"ALE/VideoPinball-v5\")` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsq5OxhTZE-3"
      },
      "source": [
        "### Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZmT7C3NZE-3"
      },
      "source": [
        "The `Video Pinball` Game form the Atari (2600) environment has the following actions which are described in the [manual of the game](https://atariage.com/manual_html_page.php?SoftwareLabelID=588):\n",
        "**This is the reduced action space, which is available when choosing `v0`, `v4` or specifying `full_action_space=false` during initialization. Otherwise more actions will be available.**\n",
        "\n",
        "| Num | Action    | Description                                                                                  |\n",
        "| --- | --------- | -------------------------------------------------------------------------------------------- |\n",
        "| 0   | NOOP      | No Operation                                                                                 |\n",
        "| 1   | FIRE      | Press the red controller button to release the spring and shoot the ball into the playfield. |\n",
        "| 2   | UP        | Move the Joystick up to move both flippers at the same time.                                 |\n",
        "| 3   | RIGHT     | Move the Joystick to the right to move the right flipper up.                                 |\n",
        "| 4   | LEFT      | Move the Joystick to the left to move the left flipper up.                                   |\n",
        "| 5   | DOWN      | Pull the Joystick down (towards you) to bring the plunger back.                              |\n",
        "| 6   | UPFIRE    | \"Nudge\" the ball into upwards direction.                                                     |\n",
        "| 7   | RIGHTFIRE | \"Nudge\" the ball to the right.                                                               |\n",
        "| 8   | LEFTFIRE  | \"Nudge\" the ball to the left.                                                                |\n",
        "\n",
        "Furthermore it might be interesting to try different modes/difficulties of the game.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aXY11AEZE-4"
      },
      "source": [
        "### Difficulties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmu_V-S3ZE-5"
      },
      "source": [
        "There are two available difficulties:\n",
        "\n",
        "- `a` (aka. pinbal wizards) is for expert players and has two additional drain holes at the bottom\n",
        "- `b` is for the beginning/novice players"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXbi9DXyZE-5"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uopuLRE6ZE-5"
      },
      "source": [
        "By default, the environment returns the RGB image which is displayed to human players as an observation.  \n",
        "However it is possible to observe\n",
        "- The 128 Bytes of RAM of the console (`Box([0 ... 0], [255 ... 255], (128,), uint8)`)\n",
        "- A grayscale image (`Box([[0 ... 0] ... [0  ... 0]], [[255 ... 255] ... [255  ... 255]], (250, 160), uint8)`)\n",
        "\n",
        "instead. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXoodpnfZE-6"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0SEDNjFZE-6"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5_4qIlyfODG",
        "outputId": "7ea5a73d-62a9-44f1-8650-990271d80643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r                                                                               \rHit:2 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rGet:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [102 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [43.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,389 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,127 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [30.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,348 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,549 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,573 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,349 kB]\n",
            "Fetched 13.8 MB in 2min 23s (96.2 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  freeglut3\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 python-opengl xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 1,355 kB of archives.\n",
            "After this operation, 8,005 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.13 [785 kB]\n",
            "Fetched 1,355 kB in 0s (12.1 MB/s)\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.13_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.13) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.13) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWU1zl7jZE-7",
        "outputId": "5fac5ad6-492e-44d7-9a28-9603298d543e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m968.6/968.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.8/760.8 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for atari_py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q --upgrade pip\n",
        "%pip install -q gym==0.21.0\n",
        "%pip install -q 'gym[atari]==0.12.5'\n",
        "%pip install -q matplotlib\n",
        "%pip install -q pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f7F9DYhOUy-"
      },
      "source": [
        "### Download required files from Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X31W6SxqO2Rg",
        "outputId": "bd28764c-175c-4806-be77-a35caeb65e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder 1sLRDYhYwlE9uNs5beDdNyrO80oBs3nzI __pycache__\n",
            "Processing file 14ST9RPnaFI-rgaHgmwQrG7LDEy3Z-OPs abstract_agent.py\n",
            "Processing file 1uOowZtnqB-Df3n5nhoSCoXNPBod40ZKk atari_helpers.py\n",
            "Processing file 1xi6hZIOR_R_eQO9het6sy2m7h2GRheu2 check_test.py\n",
            "Processing file 1Lw5_R_Y0Gk1nNSeG264I9M1w1N1WCyMF loggers.py\n",
            "Processing file 1HQHDLpU7p3YI1Av_jFunScokmy10jgTX plot_utils.py\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14ST9RPnaFI-rgaHgmwQrG7LDEy3Z-OPs\n",
            "To: /content/external/abstract_agent.py\n",
            "100%|██████████| 835/835 [00:00<00:00, 1.94MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uOowZtnqB-Df3n5nhoSCoXNPBod40ZKk\n",
            "To: /content/external/atari_helpers.py\n",
            "100%|██████████| 9.11k/9.11k [00:00<00:00, 18.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xi6hZIOR_R_eQO9het6sy2m7h2GRheu2\n",
            "To: /content/external/check_test.py\n",
            "100%|██████████| 1.56k/1.56k [00:00<00:00, 3.23MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Lw5_R_Y0Gk1nNSeG264I9M1w1N1WCyMF\n",
            "To: /content/external/loggers.py\n",
            "100%|██████████| 982/982 [00:00<00:00, 2.25MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HQHDLpU7p3YI1Av_jFunScokmy10jgTX\n",
            "To: /content/external/plot_utils.py\n",
            "100%|██████████| 2.28k/2.28k [00:00<00:00, 5.28MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    %pip install -q --upgrade gdown\n",
        "    from gdown import download_folder\n",
        "\n",
        "    download_folder(\"https://drive.google.com/drive/folders/1SW56nbccfHJtC6oGBIcp7XCeJDkKehGK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhpjsB9bZE-8"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTdn0YfeZE-8",
        "outputId": "1fa544df-e005-428c-f9bb-3ffb4207fdc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import deque\n",
        "from contextlib import suppress\n",
        "from datetime import datetime\n",
        "from random import sample\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Model\n",
        "from keras.layers import Conv2D, Dense, Flatten, Input, Lambda, multiply\n",
        "from keras.losses import huber_loss\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "from pyvirtualdisplay import Display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# local files\n",
        "from external.abstract_agent import AbstractAgent\n",
        "from external.atari_helpers import LazyFrames, make_atari, wrap_deepmind\n",
        "from external.loggers import TensorBoardLogger, tf_summary_image\n",
        "from external.plot_utils import plot_statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OXBi9qXufV-l"
      },
      "outputs": [],
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "is_ipython = 'inline' in plt.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "    from IPython.display import SVG\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgU4OVMIZE-9"
      },
      "source": [
        "### Extended DQN-Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iaJZTcGhZE-9"
      },
      "outputs": [],
      "source": [
        "class AbstractDQNAgent(AbstractAgent):\n",
        "    __slots__ = [\n",
        "        \"action_size\",\n",
        "        \"state_size\",\n",
        "        \"gamma\",\n",
        "        \"epsilon\",\n",
        "        \"epsilon_decay\",\n",
        "        \"epsilon_min\",\n",
        "        \"alpha\",\n",
        "        \"batch_size\",\n",
        "        \"memory_size\",\n",
        "        \"start_replay_step\",\n",
        "        \"target_model_update_interval\",\n",
        "        \"train_freq\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self,\n",
        "                 action_size: int,\n",
        "                 state_size: int,\n",
        "                 gamma: float,\n",
        "                 epsilon: float,\n",
        "                 epsilon_decay: float,\n",
        "                 epsilon_min: float,\n",
        "                 alpha: float,\n",
        "                 batch_size: int,\n",
        "                 memory_size: int,\n",
        "                 start_replay_step: int,\n",
        "                 target_model_update_interval: int,\n",
        "                 train_freq: int,\n",
        "                 ):\n",
        "        self.action_size = action_size\n",
        "        self.state_size = state_size\n",
        "\n",
        "        self.replay_has_started = False\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.memory_size = memory_size\n",
        "        self.memory = deque(maxlen=self.memory_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.step = 0\n",
        "        self.start_replay_step = start_replay_step\n",
        "\n",
        "        self.target_model_update_interval = target_model_update_interval\n",
        "\n",
        "        self.train_freq = train_freq\n",
        "\n",
        "        assert self.start_replay_step >= self.batch_size, \"The number of steps to start replay must be at least as large as the batch size\"\n",
        "\n",
        "        self.action_mask = np.ones((1, self.action_size))\n",
        "        self.action_mask_batch = np.ones((self.batch_size, self.action_size))\n",
        "\n",
        "        self.tf_config_intra_threads = 8\n",
        "        self.tf_config_inter_threads = 4\n",
        "        self.tf_config_soft_placement = True\n",
        "        self.tf_config_allow_growth = True\n",
        "\n",
        "        config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=self.tf_config_intra_threads,\n",
        "                                inter_op_parallelism_threads=self.tf_config_inter_threads,\n",
        "                                allow_soft_placement=self.tf_config_soft_placement\n",
        "                                )\n",
        "\n",
        "        config.gpu_options.allow_growth = self.tf_config_allow_growth\n",
        "        session = tf.compat.v1.Session(config=config)\n",
        "        set_session(session)  # set this TensorFlow session as the default session for Keras\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "\n",
        "    def save(self, target_path: str) -> None:\n",
        "      \"\"\"\n",
        "        Saves the current state of the DQNAgent to some output files.\n",
        "        Together with `load` this serves as a very rudimentary checkpointing.\n",
        "      \"\"\"\n",
        "      agent_dict = {\n",
        "            \"agent_init\": {},\n",
        "            \"agent_params\": {},\n",
        "            \"tf_config\": {}\n",
        "        }\n",
        "\n",
        "      if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "\n",
        "      for slot in self.__slots__:\n",
        "          agent_dict[\"agent_init\"].update({slot: getattr(self, slot)})\n",
        "\n",
        "      agent_dict[\"agent_init\"].update({\"memory_size\": self.memory.maxlen})\n",
        "\n",
        "      for attr in [\"action_mask\", \"action_mask_batch\"]:\n",
        "          agent_dict[\"agent_params\"].update({attr: getattr(self, attr).tolist()})\n",
        "\n",
        "      agent_dict[\"agent_params\"].update({\"memory\": list(self.memory)})\n",
        "\n",
        "      for tf_config in [\n",
        "          \"tf_config_intra_threads\",\n",
        "          \"tf_config_inter_threads\",\n",
        "          \"tf_config_soft_placement\",\n",
        "          \"tf_config_allow_growth\",\n",
        "      ]:\n",
        "          agent_dict[\"tf_config\"].update({tf_config: getattr(self, tf_config)})\n",
        "\n",
        "      with open(os.path.join(target_path, \"agent.json\"), \"w\") as f:\n",
        "          json.dump(agent_dict, f)\n",
        "\n",
        "      self.model.save_weights(os.path.join(target_path, \"model.h5\"))\n",
        "      self.target_model.save_weights(os.path.join(target_path, \"target_model.h5\"))\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> \"AbstractDQNAgent\":\n",
        "      \"\"\"\n",
        "        Loads the serialized state of a DQNAgent and returns an instance of it.\n",
        "      \"\"\"\n",
        "\n",
        "      with open(os.path.join(path, \"agent.json\"), \"r\") as f:\n",
        "          agent_dict = json.load(f)\n",
        "\n",
        "      agent = cls(**agent_dict[\"agent_init\"])\n",
        "\n",
        "      agent.action_mask = np.array(agent_dict[\"agent_params\"][\"action_mask\"])\n",
        "      agent.action_mask_batch = np.array(agent_dict[\"agent_params\"][\"action_mask_batch\"])\n",
        "\n",
        "      config = tf.compat.v1.ConfigProto(\n",
        "          intra_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_intra_threads\"],\n",
        "          inter_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_inter_threads\"],\n",
        "          allow_soft_placement=agent_dict[\"tf_config\"][\"tf_config_soft_placement\"])\n",
        "\n",
        "      config.gpu_options.allow_growth = agent_dict[\"tf_config\"][\"tf_config_allow_growth\"]\n",
        "      session = tf.compat.v1.Session(config=config)\n",
        "      set_session(session)\n",
        "\n",
        "      agent.model.load_weights('model.h5')\n",
        "      agent.target_model.load_weights(\"target_model.h5\")\n",
        "\n",
        "      return agent\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, experience):\n",
        "      raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def act(self, state):\n",
        "      raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def _build_model(self) -> Model:\n",
        "      raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efDYrUf9ZE--"
      },
      "source": [
        "## Deep Q-Learning Network (DQN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qADJ-_MoZE--",
        "outputId": "8844eb29-9624-4db3-d4a9-93d8df74b140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NoopResetEnv (max 30) wrapper is used.\n",
            "MaxAndSkipEnv (skip 4) wrapper is used.\n",
            "EpisodicLifeEnv wrapper is used.\n",
            "FireResetEnv wrapper is used.\n",
            "ClipRewardEnv wrapper is used.\n",
            "FrameStack (4) wrapper is used.\n"
          ]
        }
      ],
      "source": [
        "env = make_atari(\"VideoPinball-v4\")\n",
        "env = wrap_deepmind(env, frame_stack=True) # maps frames to 84x84x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5v-6xVSZE--"
      },
      "source": [
        "### Create the DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO2DzOVAZE-_"
      },
      "source": [
        "Take the given `AbstractDQNAgent` (previously called `DQNAgent`) and add missing methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I9nQIr7xZE-_"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(AbstractDQNAgent):\n",
        "    def _build_model(self) -> Model:\n",
        "        \"\"\"Deep Q-network as defined in the DeepMind article on Nature\n",
        "        \n",
        "        Returns:\n",
        "            Model: Tensorflow Model which will be used as internal deep neural network\n",
        "        \"\"\"\n",
        "\n",
        "        atari_shape = (84, 84, 4)\n",
        "\n",
        "        # Frames from the observation\n",
        "        frames_input = Input(atari_shape, name=\"frames\")\n",
        "\n",
        "        # Actions as input\n",
        "        action_mask = Input((self.action_size,), name=\"action_mask\")\n",
        "\n",
        "        # Normalize the frames from [0, 255] to [0, 1]\n",
        "        normalized = Lambda(lambda x: x / 255.0, name=\"normalization\")(frames_input)\n",
        "\n",
        "        # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the \n",
        "        # input image and applies a rectifier nonlinearity.\"\n",
        "        # Results in an output shape of (20, 20, 16)\n",
        "        conv1 = Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(8, 8),\n",
        "            strides=(4, 4),\n",
        "            activation=\"relu\"\n",
        "        )(normalized)\n",
        "\n",
        "        # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed \n",
        "        # by a rectifier nonlinearity.\" \n",
        "        # Results in an output shape of (9, 9, 32)\n",
        "        conv2 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(4,4),\n",
        "            strides=(2,2),\n",
        "            activation=\"relu\"\n",
        "        )(conv1)\n",
        "\n",
        "        conv3 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(4,4),\n",
        "            strides=(1,1),\n",
        "            activation=\"relu\"\n",
        "        )(conv2)\n",
        "\n",
        "        # Flattening the last convolutional layer.\n",
        "        conv_flattened = Flatten()(conv3)\n",
        "\n",
        "        # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
        "        hidden = Dense(units=512, activation='relu')(conv_flattened)\n",
        "\n",
        "        # \"The output layer is a fully-connected linear layer with a single output \n",
        "        # for each valid action.\"\n",
        "        output = Dense(self.action_size)(hidden)\n",
        "\n",
        "        # Multiply the output with the action mask to get only one action output\n",
        "        filtered_output = multiply([output, action_mask])\n",
        "\n",
        "        model = Model(inputs=[frames_input, action_mask], outputs=filtered_output)\n",
        "        model.compile(loss=huber_loss, optimizer=Adam(learning_rate=self.alpha), metrics=None)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def act(self, state: LazyFrames) -> int:\n",
        "        \"\"\"Selects the action to be executed based on the given state.\n",
        "\n",
        "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
        "        epsilon, a random action is selected.\n",
        "\n",
        "        Args:\n",
        "            state [LazyFrames]: LazyFrames object representing the state based on 4 stacked observations (images)\n",
        "\n",
        "        Returns:\n",
        "            action [int]\n",
        "        \"\"\"\n",
        "\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # ! TODO self.model.predict oder self.target_model.predict?\n",
        "            q_values = self.model.predict([[np.array(state)], self.action_mask])\n",
        "            action = np.argmax(q_values)\n",
        "        return action\n",
        "\n",
        "        \n",
        "    def train(self, experience: Tuple[LazyFrames, int, LazyFrames, float, bool]):\n",
        "        \"\"\"Stores the experience in memory. If memory is full trains network by replay.\n",
        "\n",
        "        Args:\n",
        "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        \n",
        "        self.memory.append(experience)\n",
        "        \n",
        "        #  - Update epsilon as long as it is not minimal\n",
        "        #  - Update weights of the target model (syn of the two models)\n",
        "        #  - Execute replay\n",
        "        if self.step >= self.start_replay_step:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon -= self.epsilon_decay\n",
        "            if self.step % self.target_model_update_interval == 0:\n",
        "                self.target_model.set_weights(self.model.get_weights())\n",
        "            if self.step % self.train_freq == 0:\n",
        "                self._replay()\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "\n",
        "    def _replay(self) -> None:\n",
        "        \"\"\"Gets random experiences from memory for batch update of Q-function.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        states, actions, next_states, rewards, dones = [np.array(memory) for memory in zip(*sample(self.memory, self.batch_size))]\n",
        "\n",
        "        # ! Can be left out if useless\n",
        "        assert all(isinstance(x, np.ndarray) for x in (states, actions, rewards, next_states, dones)), \\\n",
        "            \"All experience batches should be of type np.ndarray.\"\n",
        "        assert states.shape == (self.batch_size, 84, 84, 4), \\\n",
        "            f\"States shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
        "        assert actions.shape == (self.batch_size,), f\"Actions shape should be: {(self.batch_size,)}\"\n",
        "        assert rewards.shape == (self.batch_size,), f\"Rewards shape should be: {(self.batch_size,)}\"\n",
        "        assert next_states.shape == (self.batch_size, 84, 84, 4), \\\n",
        "            f\"Next states shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
        "        assert dones.shape == (self.batch_size,), f\"Dones shape should be: {(self.batch_size,)}\"\n",
        "\n",
        "        # Predict the Q values of the next states. Passing ones as the action mask.\n",
        "        next_q_values = self.target_model.predict([next_states, self.action_mask_batch], verbose=0)\n",
        "\n",
        "        # Calculate the Q values.\n",
        "        # - Terminal states get the reward\n",
        "        # - Non-terminal states get reward + gamma * max next_state q_value\n",
        "        q_values = [reward + (1 - done) * self.gamma * np.max(next_q_value) for done, reward, next_q_value in zip(dones, rewards, next_q_values)]\n",
        "\n",
        "        # Create a one hot encoding of the actions (the selected action is 1 all others 0)\n",
        "        one_hot_actions = to_categorical(actions, num_classes=self.action_size)\n",
        "\n",
        "        # Create the target Q values based on the one hot encoding of the actions and the calculated Q values\n",
        "        # This can be seen as matrix multiplication\n",
        "        # q_values = [0.5, 0.7, 0.9]\n",
        "        # actions [[1. 0. 0. 0.]\n",
        "        #          [0. 0. 1. 0.]\n",
        "        #          [0. 0. 0. 1.]]\n",
        "        # output  [[0.5 0.  0.   0. ]\n",
        "        #          [0.  0.  0.7  0. ]\n",
        "        #          [0.  0.  0.9  0. ]]\n",
        "        target_q_values = np.array(q_values)[np.newaxis].T * one_hot_actions\n",
        "\n",
        "        # Fit the model with the given states and the selected actions as one hot vector and the target_q_values as y\n",
        "        self.model.fit(\n",
        "           x=[states, one_hot_actions],  # states and mask\n",
        "           y=target_q_values,  # target Q values\n",
        "           batch_size=self.batch_size,\n",
        "           verbose=0\n",
        "        )\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nkMxNKmAZE_A"
      },
      "outputs": [],
      "source": [
        "def interact_with_environment(env, agent, n_episodes=600, max_steps=1000000, train=True, verbose=True):      \n",
        "    statistics = []\n",
        "    tb_logger = TensorBoardLogger(f'./logs/run-{datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}')\n",
        "    \n",
        "    with suppress(KeyboardInterrupt):\n",
        "        total_step = 0\n",
        "        for episode in range(n_episodes):\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            state = env.reset()\n",
        "            episode_start_time = time.time()\n",
        "            episode_step = 0\n",
        "\n",
        "            while not done:\n",
        "                action = agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if train:\n",
        "                    agent.train((state, action, next_state, reward, done))\n",
        "\n",
        "                if episode == 0:\n",
        "                    # for debug purpose log every state of first episode\n",
        "                    for obs in state:\n",
        "                        tb_logger.log_image(f'state_t{episode_step}:', tf_summary_image(np.array(obs, copy=False)),\n",
        "                                            global_step=total_step)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                episode_step += 1\n",
        "            \n",
        "            total_step += episode_step\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                speed = episode_step / (time.time() - episode_start_time)\n",
        "                tb_logger.log_scalar('score', episode_reward, global_step=total_step)\n",
        "                tb_logger.log_scalar('epsilon', agent.epsilon, global_step=total_step)\n",
        "                tb_logger.log_scalar('speed', speed, global_step=total_step)\n",
        "                if verbose:\n",
        "                    print(f'episode: {episode}/{n_episodes}, score: {episode_reward}, steps: {episode_step}, '\n",
        "                          f'total steps: {total_step}, e: {agent.epsilon:.3f}, speed: {speed:.2f} steps/s')\n",
        "\n",
        "            statistics.append({\n",
        "                'episode': episode,\n",
        "                'score': episode_reward,\n",
        "                'steps': episode_step\n",
        "            })\n",
        "                                  \n",
        "            if total_step >= max_steps:\n",
        "                break\n",
        "        \n",
        "    return statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae5vkyyHZE_A",
        "outputId": "c06c5fa7-70b5-457d-ccf6-6561c5252a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/20000, score: 10.0, steps: 91, total steps: 91, e: 1.000, speed: 163.80 steps/s\n",
            "episode: 10/20000, score: 52.0, steps: 387, total steps: 2117, e: 1.000, speed: 124.81 steps/s\n",
            "episode: 20/20000, score: 23.0, steps: 200, total steps: 3330, e: 1.000, speed: 288.12 steps/s\n",
            "episode: 30/20000, score: 22.0, steps: 99, total steps: 5477, e: 1.000, speed: 310.10 steps/s\n",
            "episode: 40/20000, score: 11.0, steps: 163, total steps: 7900, e: 1.000, speed: 305.14 steps/s\n",
            "episode: 50/20000, score: 1.0, steps: 88, total steps: 9544, e: 1.000, speed: 306.25 steps/s\n",
            "episode: 60/20000, score: 1.0, steps: 45, total steps: 11113, e: 1.000, speed: 306.62 steps/s\n",
            "episode: 70/20000, score: 7.0, steps: 135, total steps: 13366, e: 1.000, speed: 306.62 steps/s\n",
            "episode: 80/20000, score: 6.0, steps: 69, total steps: 14750, e: 1.000, speed: 290.98 steps/s\n",
            "episode: 90/20000, score: 45.0, steps: 199, total steps: 16199, e: 1.000, speed: 299.54 steps/s\n",
            "episode: 100/20000, score: 10.0, steps: 130, total steps: 17638, e: 1.000, speed: 307.22 steps/s\n",
            "episode: 110/20000, score: 7.0, steps: 95, total steps: 19200, e: 1.000, speed: 293.43 steps/s\n",
            "episode: 120/20000, score: 0.0, steps: 26, total steps: 20777, e: 1.000, speed: 324.14 steps/s\n",
            "episode: 130/20000, score: 19.0, steps: 84, total steps: 23255, e: 1.000, speed: 300.09 steps/s\n",
            "episode: 140/20000, score: 10.0, steps: 83, total steps: 25165, e: 1.000, speed: 296.54 steps/s\n",
            "episode: 150/20000, score: 12.0, steps: 239, total steps: 26592, e: 1.000, speed: 287.07 steps/s\n",
            "episode: 160/20000, score: 6.0, steps: 153, total steps: 28045, e: 1.000, speed: 309.69 steps/s\n",
            "episode: 170/20000, score: 21.0, steps: 190, total steps: 28951, e: 1.000, speed: 295.48 steps/s\n",
            "episode: 180/20000, score: 32.0, steps: 245, total steps: 31015, e: 1.000, speed: 292.17 steps/s\n",
            "episode: 190/20000, score: 3.0, steps: 39, total steps: 33058, e: 1.000, speed: 287.16 steps/s\n",
            "episode: 200/20000, score: 24.0, steps: 177, total steps: 35328, e: 1.000, speed: 307.59 steps/s\n",
            "episode: 210/20000, score: 10.0, steps: 48, total steps: 37384, e: 1.000, speed: 308.07 steps/s\n",
            "episode: 220/20000, score: 10.0, steps: 135, total steps: 39396, e: 1.000, speed: 291.98 steps/s\n",
            "episode: 230/20000, score: 23.0, steps: 159, total steps: 40597, e: 1.000, speed: 303.39 steps/s\n",
            "episode: 240/20000, score: 12.0, steps: 225, total steps: 42486, e: 1.000, speed: 301.48 steps/s\n",
            "episode: 250/20000, score: 0.0, steps: 144, total steps: 44737, e: 1.000, speed: 293.34 steps/s\n",
            "episode: 260/20000, score: 37.0, steps: 220, total steps: 46115, e: 1.000, speed: 315.66 steps/s\n",
            "episode: 270/20000, score: 34.0, steps: 271, total steps: 47612, e: 1.000, speed: 308.81 steps/s\n",
            "episode: 280/20000, score: 15.0, steps: 166, total steps: 49377, e: 1.000, speed: 297.42 steps/s\n",
            "episode: 290/20000, score: 6.0, steps: 74, total steps: 50737, e: 1.000, speed: 275.57 steps/s\n",
            "episode: 300/20000, score: 13.0, steps: 102, total steps: 52649, e: 1.000, speed: 302.91 steps/s\n",
            "episode: 310/20000, score: 21.0, steps: 178, total steps: 54473, e: 1.000, speed: 297.43 steps/s\n",
            "episode: 320/20000, score: 3.0, steps: 128, total steps: 55761, e: 1.000, speed: 302.79 steps/s\n",
            "episode: 330/20000, score: 7.0, steps: 41, total steps: 58064, e: 1.000, speed: 311.23 steps/s\n",
            "episode: 340/20000, score: 17.0, steps: 153, total steps: 59484, e: 1.000, speed: 297.65 steps/s\n",
            "episode: 350/20000, score: 8.0, steps: 64, total steps: 61352, e: 1.000, speed: 300.10 steps/s\n",
            "episode: 360/20000, score: 1.0, steps: 148, total steps: 63630, e: 1.000, speed: 312.82 steps/s\n",
            "episode: 370/20000, score: 28.0, steps: 385, total steps: 89889, e: 1.000, speed: 303.03 steps/s\n",
            "episode: 380/20000, score: 3.0, steps: 53, total steps: 92198, e: 1.000, speed: 291.37 steps/s\n",
            "episode: 390/20000, score: 46.0, steps: 357, total steps: 94254, e: 1.000, speed: 290.47 steps/s\n",
            "episode: 400/20000, score: 41.0, steps: 293, total steps: 96713, e: 1.000, speed: 301.43 steps/s\n",
            "episode: 410/20000, score: 9.0, steps: 74, total steps: 98334, e: 1.000, speed: 300.34 steps/s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 420/20000, score: 80.0, steps: 513, total steps: 100139, e: 0.999, speed: 45.10 steps/s\n",
            "episode: 430/20000, score: 0.0, steps: 132, total steps: 101162, e: 0.995, speed: 111.38 steps/s\n",
            "episode: 440/20000, score: 1.0, steps: 54, total steps: 102829, e: 0.989, speed: 76.52 steps/s\n",
            "episode: 450/20000, score: 16.0, steps: 64, total steps: 104875, e: 0.980, speed: 121.41 steps/s\n",
            "episode: 460/20000, score: 13.0, steps: 94, total steps: 106765, e: 0.973, speed: 115.79 steps/s\n",
            "episode: 470/20000, score: 117.0, steps: 628, total steps: 108922, e: 0.964, speed: 123.64 steps/s\n",
            "episode: 480/20000, score: 31.0, steps: 226, total steps: 110530, e: 0.958, speed: 121.27 steps/s\n",
            "episode: 490/20000, score: 21.0, steps: 125, total steps: 112445, e: 0.950, speed: 124.67 steps/s\n",
            "episode: 500/20000, score: 6.0, steps: 54, total steps: 113772, e: 0.945, speed: 127.08 steps/s\n",
            "episode: 510/20000, score: 19.0, steps: 149, total steps: 115575, e: 0.938, speed: 121.31 steps/s\n",
            "episode: 520/20000, score: 10.0, steps: 155, total steps: 117633, e: 0.929, speed: 119.25 steps/s\n",
            "episode: 530/20000, score: 11.0, steps: 208, total steps: 120071, e: 0.920, speed: 125.17 steps/s\n",
            "episode: 540/20000, score: 10.0, steps: 145, total steps: 121215, e: 0.915, speed: 119.89 steps/s\n",
            "episode: 550/20000, score: 54.0, steps: 328, total steps: 123112, e: 0.908, speed: 123.02 steps/s\n",
            "episode: 560/20000, score: 9.0, steps: 71, total steps: 124847, e: 0.901, speed: 115.29 steps/s\n",
            "episode: 570/20000, score: 0.0, steps: 22, total steps: 126329, e: 0.895, speed: 112.25 steps/s\n",
            "episode: 580/20000, score: 2.0, steps: 15, total steps: 127141, e: 0.891, speed: 118.48 steps/s\n",
            "episode: 590/20000, score: 45.0, steps: 379, total steps: 128685, e: 0.885, speed: 121.70 steps/s\n",
            "episode: 600/20000, score: 13.0, steps: 111, total steps: 130070, e: 0.880, speed: 118.29 steps/s\n",
            "episode: 610/20000, score: 0.0, steps: 78, total steps: 131339, e: 0.875, speed: 118.83 steps/s\n",
            "episode: 620/20000, score: 42.0, steps: 288, total steps: 133502, e: 0.866, speed: 122.25 steps/s\n",
            "episode: 630/20000, score: 17.0, steps: 162, total steps: 135273, e: 0.859, speed: 119.92 steps/s\n",
            "episode: 640/20000, score: 28.0, steps: 227, total steps: 136578, e: 0.854, speed: 119.16 steps/s\n",
            "episode: 650/20000, score: 21.0, steps: 146, total steps: 138107, e: 0.848, speed: 117.41 steps/s\n",
            "episode: 660/20000, score: 21.0, steps: 188, total steps: 139951, e: 0.840, speed: 120.05 steps/s\n",
            "episode: 670/20000, score: 37.0, steps: 268, total steps: 141672, e: 0.833, speed: 122.35 steps/s\n",
            "episode: 680/20000, score: 9.0, steps: 60, total steps: 142963, e: 0.828, speed: 120.51 steps/s\n",
            "episode: 690/20000, score: 0.0, steps: 25, total steps: 145338, e: 0.819, speed: 122.75 steps/s\n",
            "episode: 700/20000, score: 8.0, steps: 50, total steps: 146562, e: 0.814, speed: 113.31 steps/s\n",
            "episode: 710/20000, score: 5.0, steps: 59, total steps: 147672, e: 0.809, speed: 118.10 steps/s\n",
            "episode: 720/20000, score: 3.0, steps: 112, total steps: 150292, e: 0.799, speed: 117.31 steps/s\n",
            "episode: 730/20000, score: 42.0, steps: 298, total steps: 152641, e: 0.789, speed: 86.88 steps/s\n",
            "episode: 740/20000, score: 33.0, steps: 192, total steps: 154090, e: 0.784, speed: 119.17 steps/s\n",
            "episode: 750/20000, score: 2.0, steps: 30, total steps: 155440, e: 0.778, speed: 122.45 steps/s\n",
            "episode: 760/20000, score: 33.0, steps: 230, total steps: 156955, e: 0.772, speed: 119.55 steps/s\n",
            "episode: 770/20000, score: 2.0, steps: 56, total steps: 158337, e: 0.767, speed: 116.21 steps/s\n",
            "episode: 780/20000, score: 12.0, steps: 88, total steps: 160109, e: 0.760, speed: 116.01 steps/s\n",
            "episode: 790/20000, score: 12.0, steps: 92, total steps: 161249, e: 0.755, speed: 117.61 steps/s\n",
            "episode: 800/20000, score: 9.0, steps: 76, total steps: 162788, e: 0.749, speed: 112.77 steps/s\n",
            "episode: 810/20000, score: 21.0, steps: 234, total steps: 164138, e: 0.743, speed: 117.67 steps/s\n",
            "episode: 820/20000, score: 7.0, steps: 50, total steps: 165273, e: 0.739, speed: 115.15 steps/s\n",
            "episode: 830/20000, score: 0.0, steps: 47, total steps: 166298, e: 0.735, speed: 113.18 steps/s\n",
            "episode: 840/20000, score: 12.0, steps: 161, total steps: 167808, e: 0.729, speed: 111.97 steps/s\n",
            "episode: 850/20000, score: 2.0, steps: 157, total steps: 169261, e: 0.723, speed: 113.03 steps/s\n",
            "episode: 860/20000, score: 20.0, steps: 134, total steps: 170626, e: 0.717, speed: 115.41 steps/s\n",
            "episode: 870/20000, score: 9.0, steps: 176, total steps: 171962, e: 0.712, speed: 117.37 steps/s\n",
            "episode: 880/20000, score: 0.0, steps: 84, total steps: 172835, e: 0.709, speed: 114.03 steps/s\n",
            "episode: 890/20000, score: 49.0, steps: 331, total steps: 174473, e: 0.702, speed: 116.76 steps/s\n",
            "episode: 900/20000, score: 7.0, steps: 130, total steps: 175714, e: 0.697, speed: 114.96 steps/s\n",
            "episode: 910/20000, score: 3.0, steps: 93, total steps: 176903, e: 0.692, speed: 114.98 steps/s\n",
            "episode: 920/20000, score: 7.0, steps: 144, total steps: 178182, e: 0.687, speed: 111.17 steps/s\n",
            "episode: 930/20000, score: 5.0, steps: 36, total steps: 179237, e: 0.683, speed: 117.62 steps/s\n",
            "episode: 940/20000, score: 0.0, steps: 196, total steps: 180557, e: 0.678, speed: 107.70 steps/s\n",
            "episode: 950/20000, score: 14.0, steps: 182, total steps: 182198, e: 0.671, speed: 110.39 steps/s\n",
            "episode: 960/20000, score: 30.0, steps: 318, total steps: 184187, e: 0.663, speed: 112.83 steps/s\n",
            "episode: 970/20000, score: 4.0, steps: 51, total steps: 185398, e: 0.658, speed: 111.91 steps/s\n",
            "episode: 980/20000, score: 4.0, steps: 212, total steps: 186428, e: 0.654, speed: 111.45 steps/s\n",
            "episode: 990/20000, score: 24.0, steps: 193, total steps: 188047, e: 0.648, speed: 111.29 steps/s\n",
            "episode: 1000/20000, score: 22.0, steps: 413, total steps: 189674, e: 0.641, speed: 113.65 steps/s\n",
            "episode: 1010/20000, score: 48.0, steps: 323, total steps: 192115, e: 0.632, speed: 108.32 steps/s\n",
            "episode: 1020/20000, score: 3.0, steps: 33, total steps: 193735, e: 0.625, speed: 111.03 steps/s\n",
            "episode: 1030/20000, score: 7.0, steps: 33, total steps: 195092, e: 0.620, speed: 113.55 steps/s\n",
            "episode: 1040/20000, score: 0.0, steps: 115, total steps: 196420, e: 0.614, speed: 114.52 steps/s\n",
            "episode: 1050/20000, score: 14.0, steps: 165, total steps: 197371, e: 0.611, speed: 61.39 steps/s\n",
            "episode: 1060/20000, score: 1.0, steps: 149, total steps: 198442, e: 0.606, speed: 113.99 steps/s\n",
            "episode: 1070/20000, score: 66.0, steps: 452, total steps: 199438, e: 0.602, speed: 108.79 steps/s\n",
            "episode: 1080/20000, score: 11.0, steps: 186, total steps: 200703, e: 0.597, speed: 106.53 steps/s\n",
            "episode: 1090/20000, score: 11.0, steps: 157, total steps: 201814, e: 0.593, speed: 107.71 steps/s\n",
            "episode: 1100/20000, score: 14.0, steps: 169, total steps: 202937, e: 0.588, speed: 105.91 steps/s\n",
            "episode: 1110/20000, score: 3.0, steps: 72, total steps: 203877, e: 0.584, speed: 105.37 steps/s\n",
            "episode: 1120/20000, score: 4.0, steps: 79, total steps: 205091, e: 0.580, speed: 103.41 steps/s\n",
            "episode: 1130/20000, score: 4.0, steps: 110, total steps: 205922, e: 0.576, speed: 105.55 steps/s\n",
            "episode: 1140/20000, score: 31.0, steps: 378, total steps: 207413, e: 0.570, speed: 107.71 steps/s\n",
            "episode: 1150/20000, score: 51.0, steps: 329, total steps: 209208, e: 0.563, speed: 110.24 steps/s\n",
            "episode: 1160/20000, score: 0.0, steps: 74, total steps: 210462, e: 0.558, speed: 102.82 steps/s\n",
            "episode: 1170/20000, score: 10.0, steps: 69, total steps: 211605, e: 0.554, speed: 104.89 steps/s\n",
            "episode: 1180/20000, score: 27.0, steps: 262, total steps: 213155, e: 0.547, speed: 108.87 steps/s\n",
            "episode: 1190/20000, score: 20.0, steps: 265, total steps: 214619, e: 0.542, speed: 104.74 steps/s\n",
            "episode: 1200/20000, score: 6.0, steps: 59, total steps: 215927, e: 0.536, speed: 106.55 steps/s\n",
            "episode: 1210/20000, score: 12.0, steps: 146, total steps: 217511, e: 0.530, speed: 103.98 steps/s\n",
            "episode: 1220/20000, score: 2.0, steps: 86, total steps: 219000, e: 0.524, speed: 112.17 steps/s\n",
            "episode: 1230/20000, score: 0.0, steps: 1, total steps: 220084, e: 0.520, speed: 176.56 steps/s\n",
            "episode: 1240/20000, score: 10.0, steps: 75, total steps: 221149, e: 0.515, speed: 106.64 steps/s\n",
            "episode: 1250/20000, score: 5.0, steps: 184, total steps: 222206, e: 0.511, speed: 103.59 steps/s\n",
            "episode: 1260/20000, score: 17.0, steps: 130, total steps: 223442, e: 0.506, speed: 101.65 steps/s\n",
            "episode: 1270/20000, score: 23.0, steps: 259, total steps: 225554, e: 0.498, speed: 103.28 steps/s\n",
            "episode: 1280/20000, score: 6.0, steps: 128, total steps: 227051, e: 0.492, speed: 100.90 steps/s\n",
            "episode: 1290/20000, score: 4.0, steps: 109, total steps: 228191, e: 0.487, speed: 103.86 steps/s\n",
            "episode: 1300/20000, score: 14.0, steps: 98, total steps: 229680, e: 0.481, speed: 106.10 steps/s\n",
            "episode: 1310/20000, score: 15.0, steps: 82, total steps: 230725, e: 0.477, speed: 101.70 steps/s\n",
            "episode: 1320/20000, score: 32.0, steps: 184, total steps: 231906, e: 0.472, speed: 106.61 steps/s\n",
            "episode: 1330/20000, score: 10.0, steps: 131, total steps: 233527, e: 0.466, speed: 101.69 steps/s\n",
            "episode: 1340/20000, score: 14.0, steps: 209, total steps: 234613, e: 0.462, speed: 106.16 steps/s\n",
            "episode: 1350/20000, score: 43.0, steps: 219, total steps: 236073, e: 0.456, speed: 107.66 steps/s\n",
            "episode: 1360/20000, score: 3.0, steps: 93, total steps: 237378, e: 0.450, speed: 107.44 steps/s\n",
            "episode: 1370/20000, score: 36.0, steps: 241, total steps: 238837, e: 0.445, speed: 62.61 steps/s\n",
            "episode: 1380/20000, score: 10.0, steps: 74, total steps: 240024, e: 0.440, speed: 105.10 steps/s\n",
            "episode: 1390/20000, score: 0.0, steps: 126, total steps: 241612, e: 0.434, speed: 106.92 steps/s\n",
            "episode: 1400/20000, score: 10.0, steps: 161, total steps: 242757, e: 0.429, speed: 104.18 steps/s\n",
            "episode: 1410/20000, score: 8.0, steps: 108, total steps: 244110, e: 0.424, speed: 102.82 steps/s\n",
            "episode: 1420/20000, score: 3.0, steps: 90, total steps: 245308, e: 0.419, speed: 106.25 steps/s\n",
            "episode: 1430/20000, score: 8.0, steps: 216, total steps: 247191, e: 0.411, speed: 105.21 steps/s\n",
            "episode: 1440/20000, score: 5.0, steps: 68, total steps: 248533, e: 0.406, speed: 103.84 steps/s\n",
            "episode: 1450/20000, score: 1.0, steps: 128, total steps: 250129, e: 0.399, speed: 107.04 steps/s\n",
            "episode: 1460/20000, score: 2.0, steps: 122, total steps: 251601, e: 0.394, speed: 103.87 steps/s\n",
            "episode: 1470/20000, score: 14.0, steps: 115, total steps: 252969, e: 0.388, speed: 102.47 steps/s\n",
            "episode: 1480/20000, score: 0.0, steps: 51, total steps: 254322, e: 0.383, speed: 104.50 steps/s\n",
            "episode: 1490/20000, score: 6.0, steps: 153, total steps: 256051, e: 0.376, speed: 105.00 steps/s\n",
            "episode: 1500/20000, score: 14.0, steps: 122, total steps: 256987, e: 0.372, speed: 105.07 steps/s\n",
            "episode: 1510/20000, score: 3.0, steps: 119, total steps: 258739, e: 0.365, speed: 101.14 steps/s\n",
            "episode: 1520/20000, score: 7.0, steps: 27, total steps: 259881, e: 0.360, speed: 105.14 steps/s\n",
            "episode: 1530/20000, score: 7.0, steps: 106, total steps: 261409, e: 0.354, speed: 101.04 steps/s\n",
            "episode: 1540/20000, score: 10.0, steps: 75, total steps: 263260, e: 0.347, speed: 107.79 steps/s\n",
            "episode: 1550/20000, score: 16.0, steps: 183, total steps: 264187, e: 0.343, speed: 102.14 steps/s\n",
            "episode: 1560/20000, score: 15.0, steps: 77, total steps: 266051, e: 0.336, speed: 104.36 steps/s\n",
            "episode: 1570/20000, score: 20.0, steps: 106, total steps: 267479, e: 0.330, speed: 103.98 steps/s\n",
            "episode: 1580/20000, score: 0.0, steps: 78, total steps: 268375, e: 0.326, speed: 102.79 steps/s\n",
            "episode: 1590/20000, score: 3.0, steps: 19, total steps: 269360, e: 0.323, speed: 104.08 steps/s\n",
            "episode: 1600/20000, score: 9.0, steps: 100, total steps: 270549, e: 0.318, speed: 102.20 steps/s\n",
            "episode: 1610/20000, score: 2.0, steps: 26, total steps: 272376, e: 0.310, speed: 104.65 steps/s\n",
            "episode: 1620/20000, score: 0.0, steps: 17, total steps: 273804, e: 0.305, speed: 105.16 steps/s\n",
            "episode: 1630/20000, score: 5.0, steps: 140, total steps: 275927, e: 0.296, speed: 101.04 steps/s\n",
            "episode: 1640/20000, score: 13.0, steps: 233, total steps: 277701, e: 0.289, speed: 100.74 steps/s\n",
            "episode: 1650/20000, score: 5.0, steps: 117, total steps: 278864, e: 0.285, speed: 104.92 steps/s\n",
            "episode: 1660/20000, score: 55.0, steps: 455, total steps: 280936, e: 0.276, speed: 100.90 steps/s\n",
            "episode: 1670/20000, score: 0.0, steps: 63, total steps: 282881, e: 0.268, speed: 96.33 steps/s\n",
            "episode: 1680/20000, score: 28.0, steps: 326, total steps: 284614, e: 0.262, speed: 101.33 steps/s\n",
            "episode: 1690/20000, score: 20.0, steps: 220, total steps: 286184, e: 0.255, speed: 101.50 steps/s\n",
            "episode: 1700/20000, score: 3.0, steps: 125, total steps: 287112, e: 0.252, speed: 99.20 steps/s\n",
            "episode: 1710/20000, score: 19.0, steps: 80, total steps: 288192, e: 0.247, speed: 100.49 steps/s\n",
            "episode: 1720/20000, score: 7.0, steps: 34, total steps: 289462, e: 0.242, speed: 102.56 steps/s\n",
            "episode: 1730/20000, score: 6.0, steps: 66, total steps: 291182, e: 0.235, speed: 102.23 steps/s\n",
            "episode: 1740/20000, score: 14.0, steps: 87, total steps: 292430, e: 0.230, speed: 101.86 steps/s\n",
            "episode: 1750/20000, score: 1.0, steps: 47, total steps: 293522, e: 0.226, speed: 104.31 steps/s\n",
            "episode: 1760/20000, score: 2.0, steps: 41, total steps: 294567, e: 0.222, speed: 101.67 steps/s\n",
            "episode: 1770/20000, score: 33.0, steps: 247, total steps: 296568, e: 0.214, speed: 102.03 steps/s\n",
            "episode: 1780/20000, score: 8.0, steps: 135, total steps: 298625, e: 0.205, speed: 98.51 steps/s\n",
            "episode: 1790/20000, score: 10.0, steps: 59, total steps: 299904, e: 0.200, speed: 103.75 steps/s\n",
            "episode: 1800/20000, score: 5.0, steps: 127, total steps: 301458, e: 0.194, speed: 99.13 steps/s\n",
            "episode: 1810/20000, score: 7.0, steps: 63, total steps: 302826, e: 0.189, speed: 99.75 steps/s\n",
            "episode: 1820/20000, score: 5.0, steps: 54, total steps: 303903, e: 0.184, speed: 100.42 steps/s\n",
            "episode: 1830/20000, score: 0.0, steps: 62, total steps: 305420, e: 0.178, speed: 100.21 steps/s\n",
            "episode: 1840/20000, score: 1.0, steps: 46, total steps: 306468, e: 0.174, speed: 98.52 steps/s\n",
            "episode: 1850/20000, score: 0.0, steps: 76, total steps: 307849, e: 0.169, speed: 102.61 steps/s\n",
            "episode: 1860/20000, score: 12.0, steps: 77, total steps: 308814, e: 0.165, speed: 98.55 steps/s\n",
            "episode: 1870/20000, score: 16.0, steps: 121, total steps: 309775, e: 0.161, speed: 99.15 steps/s\n",
            "episode: 1880/20000, score: 58.0, steps: 335, total steps: 311321, e: 0.155, speed: 97.89 steps/s\n",
            "episode: 1890/20000, score: 11.0, steps: 190, total steps: 312445, e: 0.150, speed: 95.40 steps/s\n",
            "episode: 1900/20000, score: 9.0, steps: 113, total steps: 314080, e: 0.144, speed: 98.92 steps/s\n",
            "episode: 1910/20000, score: 3.0, steps: 312, total steps: 315477, e: 0.138, speed: 96.66 steps/s\n",
            "episode: 1920/20000, score: 0.0, steps: 39, total steps: 316883, e: 0.132, speed: 95.51 steps/s\n",
            "episode: 1930/20000, score: 6.0, steps: 167, total steps: 318304, e: 0.127, speed: 98.91 steps/s\n",
            "episode: 1940/20000, score: 27.0, steps: 335, total steps: 320327, e: 0.119, speed: 98.76 steps/s\n",
            "episode: 1950/20000, score: 5.0, steps: 204, total steps: 321877, e: 0.112, speed: 97.67 steps/s\n",
            "episode: 1960/20000, score: 5.0, steps: 81, total steps: 322949, e: 0.108, speed: 96.92 steps/s\n",
            "episode: 1970/20000, score: 4.0, steps: 48, total steps: 323997, e: 0.104, speed: 98.96 steps/s\n",
            "episode: 1980/20000, score: 17.0, steps: 122, total steps: 325438, e: 0.098, speed: 94.12 steps/s\n",
            "episode: 1990/20000, score: 11.0, steps: 151, total steps: 326757, e: 0.093, speed: 99.48 steps/s\n",
            "episode: 2000/20000, score: 0.0, steps: 27, total steps: 327897, e: 0.088, speed: 97.98 steps/s\n",
            "episode: 2010/20000, score: 0.0, steps: 353, total steps: 329154, e: 0.083, speed: 97.99 steps/s\n",
            "episode: 2020/20000, score: 9.0, steps: 117, total steps: 331222, e: 0.075, speed: 97.48 steps/s\n",
            "episode: 2030/20000, score: 3.0, steps: 134, total steps: 332402, e: 0.070, speed: 98.73 steps/s\n",
            "episode: 2040/20000, score: 44.0, steps: 312, total steps: 334484, e: 0.062, speed: 98.46 steps/s\n",
            "episode: 2050/20000, score: 5.0, steps: 68, total steps: 335494, e: 0.058, speed: 99.63 steps/s\n",
            "episode: 2060/20000, score: 41.0, steps: 345, total steps: 336733, e: 0.053, speed: 96.57 steps/s\n",
            "episode: 2070/20000, score: 11.0, steps: 119, total steps: 338195, e: 0.047, speed: 96.70 steps/s\n",
            "episode: 2080/20000, score: 1.0, steps: 4, total steps: 339292, e: 0.043, speed: 96.97 steps/s\n",
            "episode: 2090/20000, score: 7.0, steps: 32, total steps: 340288, e: 0.039, speed: 95.68 steps/s\n",
            "episode: 2100/20000, score: 7.0, steps: 101, total steps: 341631, e: 0.033, speed: 92.42 steps/s\n",
            "episode: 2110/20000, score: 6.0, steps: 103, total steps: 343099, e: 0.028, speed: 59.96 steps/s\n",
            "episode: 2120/20000, score: 12.0, steps: 140, total steps: 344610, e: 0.022, speed: 98.00 steps/s\n",
            "episode: 2130/20000, score: 3.0, steps: 51, total steps: 345567, e: 0.018, speed: 95.68 steps/s\n",
            "episode: 2140/20000, score: 5.0, steps: 304, total steps: 347371, e: 0.011, speed: 96.22 steps/s\n",
            "episode: 2150/20000, score: 13.0, steps: 79, total steps: 349406, e: 0.010, speed: 93.33 steps/s\n",
            "episode: 2160/20000, score: 13.0, steps: 75, total steps: 350705, e: 0.010, speed: 98.10 steps/s\n",
            "episode: 2170/20000, score: 9.0, steps: 185, total steps: 352324, e: 0.010, speed: 96.05 steps/s\n",
            "episode: 2180/20000, score: 10.0, steps: 96, total steps: 353482, e: 0.010, speed: 92.72 steps/s\n",
            "episode: 2190/20000, score: 12.0, steps: 66, total steps: 354925, e: 0.010, speed: 95.62 steps/s\n",
            "episode: 2200/20000, score: 4.0, steps: 58, total steps: 356258, e: 0.010, speed: 95.51 steps/s\n",
            "episode: 2210/20000, score: 24.0, steps: 168, total steps: 357753, e: 0.010, speed: 44.70 steps/s\n",
            "episode: 2220/20000, score: 26.0, steps: 269, total steps: 359038, e: 0.010, speed: 95.46 steps/s\n",
            "episode: 2230/20000, score: 19.0, steps: 184, total steps: 360751, e: 0.010, speed: 97.04 steps/s\n",
            "episode: 2240/20000, score: 14.0, steps: 86, total steps: 362414, e: 0.010, speed: 95.73 steps/s\n",
            "episode: 2250/20000, score: 41.0, steps: 240, total steps: 363994, e: 0.010, speed: 96.78 steps/s\n",
            "episode: 2260/20000, score: 7.0, steps: 79, total steps: 365447, e: 0.010, speed: 89.00 steps/s\n",
            "episode: 2270/20000, score: 5.0, steps: 130, total steps: 366874, e: 0.010, speed: 96.42 steps/s\n",
            "episode: 2280/20000, score: 0.0, steps: 121, total steps: 368456, e: 0.010, speed: 96.59 steps/s\n",
            "episode: 2290/20000, score: 3.0, steps: 43, total steps: 369604, e: 0.010, speed: 70.15 steps/s\n",
            "episode: 2300/20000, score: 8.0, steps: 94, total steps: 370841, e: 0.010, speed: 96.66 steps/s\n",
            "episode: 2310/20000, score: 1.0, steps: 119, total steps: 372903, e: 0.010, speed: 96.41 steps/s\n",
            "episode: 2320/20000, score: 8.0, steps: 61, total steps: 374284, e: 0.010, speed: 96.53 steps/s\n",
            "episode: 2330/20000, score: 4.0, steps: 223, total steps: 375840, e: 0.010, speed: 95.52 steps/s\n",
            "episode: 2340/20000, score: 67.0, steps: 412, total steps: 377497, e: 0.010, speed: 96.00 steps/s\n",
            "episode: 2350/20000, score: 29.0, steps: 189, total steps: 380010, e: 0.010, speed: 95.26 steps/s\n",
            "episode: 2360/20000, score: 13.0, steps: 124, total steps: 381639, e: 0.010, speed: 95.64 steps/s\n",
            "episode: 2370/20000, score: 3.0, steps: 38, total steps: 382466, e: 0.010, speed: 95.24 steps/s\n",
            "episode: 2380/20000, score: 10.0, steps: 213, total steps: 384141, e: 0.010, speed: 93.69 steps/s\n",
            "episode: 2390/20000, score: 14.0, steps: 75, total steps: 385913, e: 0.010, speed: 95.92 steps/s\n",
            "episode: 2400/20000, score: 38.0, steps: 211, total steps: 387717, e: 0.010, speed: 95.79 steps/s\n",
            "episode: 2410/20000, score: 33.0, steps: 172, total steps: 389162, e: 0.010, speed: 94.85 steps/s\n",
            "episode: 2420/20000, score: 9.0, steps: 62, total steps: 390194, e: 0.010, speed: 93.79 steps/s\n",
            "episode: 2430/20000, score: 29.0, steps: 271, total steps: 391938, e: 0.010, speed: 97.12 steps/s\n",
            "episode: 2440/20000, score: 5.0, steps: 87, total steps: 393343, e: 0.010, speed: 94.02 steps/s\n",
            "episode: 2450/20000, score: 5.0, steps: 46, total steps: 395124, e: 0.010, speed: 100.56 steps/s\n",
            "episode: 2460/20000, score: 19.0, steps: 310, total steps: 397646, e: 0.010, speed: 95.26 steps/s\n",
            "episode: 2470/20000, score: 41.0, steps: 501, total steps: 399200, e: 0.010, speed: 76.55 steps/s\n",
            "episode: 2480/20000, score: 17.0, steps: 147, total steps: 400816, e: 0.010, speed: 96.08 steps/s\n",
            "episode: 2490/20000, score: 9.0, steps: 101, total steps: 402449, e: 0.010, speed: 94.66 steps/s\n",
            "episode: 2500/20000, score: 3.0, steps: 62, total steps: 404185, e: 0.010, speed: 92.84 steps/s\n",
            "episode: 2510/20000, score: 0.0, steps: 235, total steps: 405593, e: 0.010, speed: 96.19 steps/s\n",
            "episode: 2520/20000, score: 51.0, steps: 311, total steps: 407873, e: 0.010, speed: 95.79 steps/s\n",
            "episode: 2530/20000, score: 5.0, steps: 67, total steps: 410524, e: 0.010, speed: 99.95 steps/s\n",
            "episode: 2540/20000, score: 12.0, steps: 143, total steps: 412711, e: 0.010, speed: 97.19 steps/s\n",
            "episode: 2550/20000, score: 16.0, steps: 296, total steps: 414055, e: 0.010, speed: 95.51 steps/s\n",
            "episode: 2560/20000, score: 0.0, steps: 71, total steps: 415778, e: 0.010, speed: 94.87 steps/s\n",
            "episode: 2570/20000, score: 3.0, steps: 39, total steps: 418524, e: 0.010, speed: 99.19 steps/s\n",
            "episode: 2580/20000, score: 6.0, steps: 58, total steps: 420669, e: 0.010, speed: 94.10 steps/s\n",
            "episode: 2590/20000, score: 7.0, steps: 68, total steps: 422083, e: 0.010, speed: 97.61 steps/s\n",
            "episode: 2600/20000, score: 28.0, steps: 280, total steps: 423475, e: 0.010, speed: 96.21 steps/s\n",
            "episode: 2610/20000, score: 45.0, steps: 330, total steps: 425526, e: 0.010, speed: 94.59 steps/s\n",
            "episode: 2620/20000, score: 6.0, steps: 90, total steps: 427107, e: 0.010, speed: 98.85 steps/s\n",
            "episode: 2630/20000, score: 10.0, steps: 162, total steps: 428758, e: 0.010, speed: 53.33 steps/s\n",
            "episode: 2640/20000, score: 0.0, steps: 111, total steps: 430498, e: 0.010, speed: 94.29 steps/s\n",
            "episode: 2650/20000, score: 4.0, steps: 146, total steps: 432179, e: 0.010, speed: 98.17 steps/s\n",
            "episode: 2660/20000, score: 8.0, steps: 166, total steps: 433550, e: 0.010, speed: 94.22 steps/s\n",
            "episode: 2670/20000, score: 23.0, steps: 85, total steps: 435387, e: 0.010, speed: 94.23 steps/s\n",
            "episode: 2680/20000, score: 33.0, steps: 150, total steps: 436808, e: 0.010, speed: 97.36 steps/s\n",
            "episode: 2690/20000, score: 50.0, steps: 433, total steps: 438639, e: 0.010, speed: 96.84 steps/s\n"
          ]
        }
      ],
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "\n",
        "# Hyperparams (should be sufficient)\n",
        "episodes = 20000\n",
        "annealing_steps = 100000  # not episodes!\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.000004\n",
        "alpha = 0.0001\n",
        "batch_size = 64\n",
        "memory_size = 100000\n",
        "start_replay_step = 100000\n",
        "target_model_update_interval = 1000\n",
        "train_freq = 4\n",
        "\n",
        "agent = DQNAgent(action_size=action_size, state_size=state_size, gamma=gamma, \n",
        "                 epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min, \n",
        "                 alpha=alpha, batch_size=batch_size, memory_size=memory_size,\n",
        "                 start_replay_step=start_replay_step, \n",
        "                 target_model_update_interval=target_model_update_interval, train_freq=train_freq)\n",
        "\n",
        "statistics = interact_with_environment(env, agent, n_episodes=episodes, verbose=True)\n",
        "env.close()\n",
        "plot_statistics(statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxXdW7Biea4M"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    state = env.reset()\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    for j in range(200):\n",
        "        action = agent.act(state)\n",
        "        img.set_data(env.render(mode='rgb_array')) \n",
        "        plt.axis('off')\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            break \n",
        "            \n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHZxhRSHf6yU"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(agent.model, to_file='keras_plot_model_2.png', show_shapes=True)\n",
        "display.Image('keras_plot_model_2.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPrMm9odgFs7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from os import path\n",
        "\n",
        "save_dir = \"./saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1d6Ah1GgJ-G"
      },
      "outputs": [],
      "source": [
        "agent.model.save(path.join(save_dir, \"model.tf\"))\n",
        "agent.target_model.save(path.join(save_dir, \"target_model.tf\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjzHcEOHgaun"
      },
      "outputs": [],
      "source": [
        "agent.model = None\n",
        "agent.target_model = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU6_sPNKgb7b"
      },
      "outputs": [],
      "source": [
        "with open(path.join(save_dir, \"agent.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(agent, f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-IOIKZghSn"
      },
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxBJ3gv1ggqA"
      },
      "outputs": [],
      "source": [
        "with open(path.join(save_dir, \"agent.pkl\"), \"rb\") as f:\n",
        "    agent = pickle.load(f)\n",
        "    f.close()\n",
        "\n",
        "agent.model = tf.keras.models.load_model(path.join(save_dir, \"model.tf\"))\n",
        "agent.target_model = tf.keras.models.load_model(path.join(save_dir, \"target_model.tf\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMBpIA_oZE_A"
      },
      "source": [
        "## To-Dos\n",
        "\n",
        "- Create on place for hyperparameters for inside the model and pass it on\n",
        "  - e. g. optimizer, different metrics, checkpointing for the internal model, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5UV7EPfZE_B"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "119d84cb8c3b84b6d48a93e2933b395fcb363c6d1cda3c0de52c91f1ece8e0fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}