{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Pinball"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to teach a reinforcement learning agent to play the game [Atari Video Pinball](https://gymnasium.farama.org/environments/atari/video_pinball/).  \n",
    "For this purpose, the Deep Q-Learning approach is followed using a neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General information:\n",
    "\n",
    "|                   |                                   |\n",
    "| ----------------- | --------------------------------- |\n",
    "| Action Space      | Discrete(18)                      |\n",
    "| Observation Space | (210, 160, 3)                     |\n",
    "| Observation High  | 255                               |\n",
    "| Observation Low   | 0                                 |\n",
    "| Import            | `gym.make(\"ALE/VideoPinball-v5\")` |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Video Pinball` Game form the Atari (2600) environment has the following actions which are described in the [manual of the game](https://atariage.com/manual_html_page.php?SoftwareLabelID=588):\n",
    "**This is the reduced action space, which is available when choosing `v0`, `v4` or specifying `full_action_space=false` during initialization. Otherwise more actions will be available.**\n",
    "\n",
    "| Num | Action    | Description                                                                                  |\n",
    "| --- | --------- | -------------------------------------------------------------------------------------------- |\n",
    "| 0   | NOOP      | No Operation                                                                                 |\n",
    "| 1   | FIRE      | Press the red controller button to release the spring and shoot the ball into the playfield. |\n",
    "| 2   | UP        | Move the Joystick up to move both flippers at the same time.                                 |\n",
    "| 3   | RIGHT     | Move the Joystick to the right to move the right flipper up.                                 |\n",
    "| 4   | LEFT      | Move the Joystick to the left to move the left flipper up.                                   |\n",
    "| 5   | DOWN      | Pull the Joystick down (towards you) to bring the plunger back.                              |\n",
    "| 6   | UPFIRE    | \"Nudge\" the ball into upwards direction.                                                     |\n",
    "| 7   | RIGHTFIRE | \"Nudge\" the ball to the right.                                                               |\n",
    "| 8   | LEFTFIRE  | \"Nudge\" the ball to the left.                                                                |\n",
    "\n",
    "Furthermore it might be interesting to try different modes/difficulties of the game.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulties"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two available difficulties:\n",
    "\n",
    "- `a` (aka. pinbal wizards) is for expert players and has two additional drain holes at the bottom\n",
    "- `b` is for the beginning/novice players"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the environment returns the RGB image which is displayed to human players as an observation.  \n",
    "However it is possible to observe\n",
    "- The 128 Bytes of RAM of the console (`Box([0 ... 0], [255 ... 255], (128,), uint8)`)\n",
    "- A grayscale image (`Box([[0 ... 0] ... [0  ... 0]], [[255 ... 255] ... [255  ... 255]], (250, 160), uint8)`)\n",
    "\n",
    "instead. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (22.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium[all] in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (4.11.3)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (0.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.0.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (3.5.1)\n",
      "Requirement already satisfied: mujoco==2.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.2.0)\n",
      "Requirement already satisfied: imageio>=2.14.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.23.0)\n",
      "Requirement already satisfied: lz4>=3.1.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (3.1.3)\n",
      "Requirement already satisfied: mujoco-py<2.2,>=2.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.1.2.14)\n",
      "Requirement already satisfied: pytest==7.0.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (7.0.1)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (1.0.3)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.1.0)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (4.1.1)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.3.5)\n",
      "Requirement already satisfied: gym==0.26.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (0.26.2)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (0.8.0)\n",
      "Requirement already satisfied: opencv-python>=3.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[all]) (4.7.0.68)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gym==0.26.2->gymnasium[all]) (0.0.8)\n",
      "Requirement already satisfied: pyopengl in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from mujoco==2.2->gymnasium[all]) (3.1.6)\n",
      "Requirement already satisfied: glfw in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from mujoco==2.2->gymnasium[all]) (2.5.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from mujoco==2.2->gymnasium[all]) (1.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (21.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (1.0.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (21.4.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (1.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from pytest==7.0.1->gymnasium[all]) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from ale-py~=0.8.0->gymnasium[all]) (4.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from ale-py~=0.8.0->gymnasium[all]) (5.10.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from imageio>=2.14.1->gymnasium[all]) (9.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[all]) (3.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (4.25.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (4.64.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (0.1.10)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (4.4.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (2.27.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (0.4.7)\n",
      "Requirement already satisfied: Cython>=0.27.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (0.29.28)\n",
      "Requirement already satisfied: cffi>=1.10 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.15.0)\n",
      "Requirement already satisfied: fasteners~=0.15 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (0.18)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium[accept-rom-license] in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (1.21.5)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (4.11.3)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.27.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[accept-rom-license]) (3.8.0)\n",
      "Requirement already satisfied: libtorrent in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2022.5.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (3.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sebastian\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install gymnasium[all]\n",
    "%pip install gymnasium[accept-rom-license]\n",
    "%pip install matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sebastian\\anaconda3\\lib\\site-packages\\gymnasium\\envs\\registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from typing import Any\n",
    "from loggers import TensorBoardLogger\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, Lambda, multiply\n",
    "from keras.losses import huber_loss\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "\n",
    "# local files\n",
    "from external.abstract_agent import AbstractAgent\n",
    "from external.atari_helpers import LazyFrames, make_atari, wrap_deepmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/VideoPinball-v5\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended DQN-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractDQNAgent(AbstractAgent):\n",
    "    __slots__ = [\n",
    "        \"action_size\",\n",
    "        \"state_size\",\n",
    "        \"gamma\",\n",
    "        \"epsilon\",\n",
    "        \"epsilon_decay\",\n",
    "        \"epsilon_min\",\n",
    "        \"alpha\",\n",
    "        \"batch_size\",\n",
    "        \"memory_size\",\n",
    "        \"start_replay_step\",\n",
    "        \"target_model_update_interval\",\n",
    "        \"train_freq\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size: int,\n",
    "                 state_size: int,\n",
    "                 gamma: float,\n",
    "                 epsilon: float,\n",
    "                 epsilon_decay: float,\n",
    "                 epsilon_min: float,\n",
    "                 alpha: float,\n",
    "                 batch_size: int,\n",
    "                 memory_size: int,\n",
    "                 start_replay_step: int,\n",
    "                 target_model_update_interval: int,\n",
    "                 train_freq: int,\n",
    "                 ):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.step = 0\n",
    "        self.start_replay_step = start_replay_step\n",
    "\n",
    "        self.target_model_update_interval = target_model_update_interval\n",
    "\n",
    "        self.train_freq = train_freq\n",
    "\n",
    "        assert self.start_replay_step >= self.batch_size, \"The number of steps to start replay must be at least as large as the batch size\"\n",
    "\n",
    "        self.action_mask = np.ones((1, self.action_size))\n",
    "        self.action_mask_batch = np.ones((self.batch_size, self.action_size))\n",
    "\n",
    "        self.tf_config_intra_threads = 8\n",
    "        self.tf_config_inter_threads = 4\n",
    "        self.tf_config_soft_placement = True\n",
    "        self.tf_config_allow_growth = True\n",
    "\n",
    "        config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=self.tf_config_intra_threads,\n",
    "                                inter_op_parallelism_threads=self.tf_config_inter_threads,\n",
    "                                allow_soft_placement=self.tf_config_soft_placement\n",
    "                                )\n",
    "\n",
    "        config.gpu_options.allow_growth = self.tf_config_allow_growth\n",
    "        session = tf.compat.v1.Session(config=config)\n",
    "        set_session(session)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "\n",
    "    def save(self, target_path: str) -> None:\n",
    "      \"\"\"\n",
    "        Saves the current state of the DQNAgent to some output files.\n",
    "        Together with `load` this serves as a very rudimentary checkpointing.\n",
    "      \"\"\"\n",
    "      agent_dict = {\n",
    "            \"agent_init\": {},\n",
    "            \"agent_params\": {},\n",
    "            \"tf_config\": {}\n",
    "        }\n",
    "\n",
    "      if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "      for slot in self.__slots__:\n",
    "          agent_dict[\"agent_init\"].update({slot: getattr(self, slot)})\n",
    "\n",
    "      agent_dict[\"agent_init\"].update({\"memory_size\": self.memory.maxlen})\n",
    "\n",
    "      for attr in [\"action_mask\", \"action_mask_batch\"]:\n",
    "          agent_dict[\"agent_params\"].update({attr: getattr(self, attr).tolist()})\n",
    "\n",
    "      agent_dict[\"agent_params\"].update({\"memory\": list(self.memory)})\n",
    "\n",
    "      for tf_config in [\n",
    "          \"tf_config_intra_threads\",\n",
    "          \"tf_config_inter_threads\",\n",
    "          \"tf_config_soft_placement\",\n",
    "          \"tf_config_allow_growth\",\n",
    "      ]:\n",
    "          agent_dict[\"tf_config\"].update({tf_config: getattr(self, tf_config)})\n",
    "\n",
    "      with open(os.path.join(target_path, \"agent.json\"), \"w\") as f:\n",
    "          json.dump(agent_dict, f)\n",
    "\n",
    "      self.model.save_weights(os.path.join(target_path, \"model.h5\"))\n",
    "      self.target_model.save_weights(os.path.join(target_path, \"target_model.h5\"))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"AbstractDQNAgent\":\n",
    "      \"\"\"\n",
    "        Loads the serialized state of a DQNAgent and returns an instance of it.\n",
    "      \"\"\"\n",
    "\n",
    "      with open(os.path.join(path, \"agent.json\"), \"r\") as f:\n",
    "          agent_dict = json.load(f)\n",
    "\n",
    "      agent = cls(**agent_dict[\"agent_init\"])\n",
    "\n",
    "      agent.action_mask = np.array(agent_dict[\"agent_params\"][\"action_mask\"])\n",
    "      agent.action_mask_batch = np.array(agent_dict[\"agent_params\"][\"action_mask_batch\"])\n",
    "\n",
    "      config = tf.compat.v1.ConfigProto(\n",
    "          intra_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_intra_threads\"],\n",
    "          inter_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_inter_threads\"],\n",
    "          allow_soft_placement=agent_dict[\"tf_config\"][\"tf_config_soft_placement\"])\n",
    "\n",
    "      config.gpu_options.allow_growth = agent_dict[\"tf_config\"][\"tf_config_allow_growth\"]\n",
    "      session = tf.compat.v1.Session(config=config)\n",
    "      set_session(session)\n",
    "\n",
    "      agent.model.load_weights('model.h5')\n",
    "      agent.target_model.load_weights(\"target_model.h5\")\n",
    "\n",
    "      return agent\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, experience):\n",
    "      raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self, state):\n",
    "      raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_model(self) -> Model:\n",
    "      raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoopResetEnv (max 30) wrapper is used.\n",
      "MaxAndSkipEnv (skip 4) wrapper is used.\n",
      "EpisodicLifeEnv wrapper is used.\n",
      "FireResetEnv wrapper is used.\n",
      "ClipRewardEnv wrapper is used.\n",
      "FrameStack (4) wrapper is used.\n"
     ]
    }
   ],
   "source": [
    "env = make_atari(\"ALE/VideoPinball-v5\")\n",
    "env = wrap_deepmind(env, frame_stack=True) # maps frames to 84x84x4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DQN Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the given `AbstractDQNAgent` (previously called `DQNAgent`) and add missing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(AbstractDQNAgent):\n",
    "    def _build_model(self) -> Model:\n",
    "        \"\"\"Deep Q-network as defined in the DeepMind article on Nature\n",
    "        \n",
    "        Returns:\n",
    "            Model: Tensorflow Model which will be used as internal deep neural network\n",
    "        \"\"\"\n",
    "\n",
    "        atari_shape = (84, 84, 4)\n",
    "\n",
    "        # Frames from the observation\n",
    "        frames_input = Input(atari_shape, name=\"frames\")\n",
    "\n",
    "        # Actions as input\n",
    "        action_mask = Input((self.action_size,), name=\"action_mask\")\n",
    "\n",
    "        # Normalize the frames from [0, 255] to [0, 1]\n",
    "        normalized = Lambda(lambda x: x / 255.0, name=\"normalization\")(frames_input)\n",
    "\n",
    "        # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the \n",
    "        # input image and applies a rectifier nonlinearity.\"\n",
    "        # Results in an output shape of (20, 20, 16)\n",
    "        conv1 = Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(8, 8),\n",
    "            strides=(4, 4),\n",
    "            activation=\"relu\"\n",
    "        )(normalized)\n",
    "\n",
    "        # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed \n",
    "        # by a rectifier nonlinearity.\" \n",
    "        # Results in an output shape of (9, 9, 32)\n",
    "        conv2 = Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(4,4),\n",
    "            strides=(2,2),\n",
    "            activation=\"relu\"\n",
    "        )(conv1)\n",
    "\n",
    "        # Flattening the last convolutional layer.\n",
    "        conv_flattened = Flatten(conv2)\n",
    "\n",
    "        # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "        hidden = Dense(units=256, activation='relu')(conv_flattened)\n",
    "\n",
    "        # \"The output layer is a fully-connected linear layer with a single output \n",
    "        # for each valid action.\"\n",
    "        output = Dense(self.action_size)(hidden)\n",
    "\n",
    "        # Multiply the output with the action mask to get only one action output\n",
    "        filtered_output = multiply([output, action_mask])\n",
    "\n",
    "        model = Model(inputs=[frames_input, action_mask], outputs=filtered_output)\n",
    "        model.compile(loss=huber_loss, optimizer=Adam(learning_rate=self.alpha), metrics=None)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def act(self, state: LazyFrames) -> int:\n",
    "        \"\"\"Selects the action to be executed based on the given state.\n",
    "\n",
    "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
    "        epsilon, a random action is selected.\n",
    "\n",
    "        Args:\n",
    "            state [LazyFrames]: LazyFrames object representing the state based on 4 stacked observations (images)\n",
    "\n",
    "        Returns:\n",
    "            action [int]\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # ! TODO self.model.predict oder self.target_model.predict?\n",
    "            q_values = self.model.predict([[np.array(state)], self.action_mask])\n",
    "            action = np.argmax(q_values)\n",
    "        return action\n",
    "\n",
    "        \n",
    "    def train(self, experience: tuple[LazyFrames, int, LazyFrames, float, bool]):\n",
    "        \"\"\"Stores the experience in memory. If memory is full trains network by replay.\n",
    "\n",
    "        Args:\n",
    "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.append(experience)\n",
    "        \n",
    "        #  - Update epsilon as long as it is not minimal\n",
    "        #  - Update weights of the target model (syn of the two models)\n",
    "        #  - Execute replay\n",
    "        if self.step >= self.start_replay_step:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decay\n",
    "            if self.step % self.target_model_update_interval == 0:\n",
    "                self.target_model.set_weights(self.model.get_weights())\n",
    "            self._replay()\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "\n",
    "    def _replay(self) -> None:\n",
    "        \"\"\"Gets random experiences from memory for batch update of Q-function.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        states, actions, next_states, rewards, dones = [np.array(memory) for memory in zip(*sample(self.memory, self.batch_size))]\n",
    "\n",
    "        # ! Can be left out if useless\n",
    "        assert all(isinstance(x, np.ndarray) for x in (states, actions, rewards, next_states, dones)), \\\n",
    "            \"All experience batches should be of type np.ndarray.\"\n",
    "        assert states.shape == (self.batch_size, 84, 84, 4), \\\n",
    "            f\"States shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
    "        assert actions.shape == (self.batch_size,), f\"Actions shape should be: {(self.batch_size,)}\"\n",
    "        assert rewards.shape == (self.batch_size,), f\"Rewards shape should be: {(self.batch_size,)}\"\n",
    "        assert next_states.shape == (self.batch_size, 84, 84, 4), \\\n",
    "            f\"Next states shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
    "        assert dones.shape == (self.batch_size,), f\"Dones shape should be: {(self.batch_size,)}\"\n",
    "\n",
    "        # Predict the Q values of the next states. Passing ones as the action mask.\n",
    "        next_q_values = self.target_model.predict([next_states, self.action_mask_batch], verbose=0)\n",
    "\n",
    "        # Calculate the Q values.\n",
    "        # - Terminal states get the reward\n",
    "        # - Non-terminal states get reward + gamma * max next_state q_value\n",
    "        q_values = [reward + (1 - done) * self.gamma * np.max(next_q_value) for done, reward, next_q_value in zip(dones, rewards, next_q_values)]\n",
    "\n",
    "        # Create a one hot encoding of the actions (the selected action is 1 all others 0)\n",
    "        one_hot_actions = to_categorical(actions, num_classes=self.action_size)\n",
    "\n",
    "        # Create the target Q values based on the one hot encoding of the actions and the calculated Q values\n",
    "        # This can be seen as matrix multiplication\n",
    "        # q_values = [0.5, 0.7, 0.9]\n",
    "        # actions [[1. 0. 0. 0.]\n",
    "        #          [0. 0. 1. 0.]\n",
    "        #          [0. 0. 0. 1.]]\n",
    "        # output  [[0.5 0.  0.   0. ]\n",
    "        #          [0.  0.  0.7  0. ]\n",
    "        #          [0.  0.  0.9  0. ]]\n",
    "        target_q_values = np.array(q_values)[np.newaxis].T * one_hot_actions\n",
    "\n",
    "        # Fit the model with the given states and the selected actions as one hot vector and the target_q_values as y\n",
    "        self.model.fit(\n",
    "           x=[states, one_hot_actions],  # states and mask\n",
    "           y=target_q_values,  # target Q values\n",
    "           batch_size=self.batch_size,\n",
    "           verbose=0\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_environment(environment, agent: AbstractDQNAgent, n_episodes=600, max_steps=1000000, train=True, verbose=True) -> list[dict[str, Any]]:\n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Dos\n",
    "\n",
    "- Create on place for hyperparameters for inside the model and pass it on\n",
    "  - e. g. optimizer, different metrics, checkpointing for the internal model, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "487b39d4bc77932302fbf00c8aa33c8cae154b5482e37c69cf95409c8a1ceaae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
