{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0CXP1j7ZE-y"
      },
      "source": [
        "# Video Pinball"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfohG30UZE-1"
      },
      "source": [
        "This project aims to teach a reinforcement learning agent to play the game [Atari Video Pinball](https://gymnasium.farama.org/environments/atari/video_pinball/).  \n",
        "For this purpose, the Deep Q-Learning approach is followed using a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oha95PZYZE-2"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uto_yJs4ZE-3"
      },
      "source": [
        "General information:\n",
        "\n",
        "|                   |                                   |\n",
        "| ----------------- | --------------------------------- |\n",
        "| Action Space      | Discrete(18)                      |\n",
        "| Observation Space | (210, 160, 3)                     |\n",
        "| Observation High  | 255                               |\n",
        "| Observation Low   | 0                                 |\n",
        "| Import            | `gym.make(\"ALE/VideoPinball-v5\")` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsq5OxhTZE-3"
      },
      "source": [
        "### Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZmT7C3NZE-3"
      },
      "source": [
        "The `Video Pinball` Game form the Atari (2600) environment has the following actions which are described in the [manual of the game](https://atariage.com/manual_html_page.php?SoftwareLabelID=588):\n",
        "**This is the reduced action space, which is available when choosing `v0`, `v4` or specifying `full_action_space=false` during initialization. Otherwise more actions will be available.**\n",
        "\n",
        "| Num | Action    | Description                                                                                  |\n",
        "| --- | --------- | -------------------------------------------------------------------------------------------- |\n",
        "| 0   | NOOP      | No Operation                                                                                 |\n",
        "| 1   | FIRE      | Press the red controller button to release the spring and shoot the ball into the playfield. |\n",
        "| 2   | UP        | Move the Joystick up to move both flippers at the same time.                                 |\n",
        "| 3   | RIGHT     | Move the Joystick to the right to move the right flipper up.                                 |\n",
        "| 4   | LEFT      | Move the Joystick to the left to move the left flipper up.                                   |\n",
        "| 5   | DOWN      | Pull the Joystick down (towards you) to bring the plunger back.                              |\n",
        "| 6   | UPFIRE    | \"Nudge\" the ball into upwards direction.                                                     |\n",
        "| 7   | RIGHTFIRE | \"Nudge\" the ball to the right.                                                               |\n",
        "| 8   | LEFTFIRE  | \"Nudge\" the ball to the left.                                                                |\n",
        "\n",
        "Furthermore it might be interesting to try different modes/difficulties of the game.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aXY11AEZE-4"
      },
      "source": [
        "### Difficulties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmu_V-S3ZE-5"
      },
      "source": [
        "There are two available difficulties:\n",
        "\n",
        "- `a` (aka. pinbal wizards) is for expert players and has two additional drain holes at the bottom\n",
        "- `b` is for the beginning/novice players"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXbi9DXyZE-5"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uopuLRE6ZE-5"
      },
      "source": [
        "By default, the environment returns the RGB image which is displayed to human players as an observation.  \n",
        "However it is possible to observe\n",
        "- The 128 Bytes of RAM of the console (`Box([0 ... 0], [255 ... 255], (128,), uint8)`)\n",
        "- A grayscale image (`Box([[0 ... 0] ... [0  ... 0]], [[255 ... 255] ... [255  ... 255]], (250, 160), uint8)`)\n",
        "\n",
        "instead. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXoodpnfZE-6"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0SEDNjFZE-6"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5_4qIlyfODG",
        "outputId": "60bf481f-297c-4605-fb2b-5ad56edf87af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.13).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWU1zl7jZE-7",
        "outputId": "c0db7be7-9e0f-4fe6-c300-c4017a69be24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q --upgrade pip\n",
        "%pip install -q gym==0.21.0\n",
        "%pip install -q 'gym[atari]==0.12.5'\n",
        "%pip install -q matplotlib\n",
        "%pip install -q pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f7F9DYhOUy-"
      },
      "source": [
        "### Download required files from Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X31W6SxqO2Rg",
        "outputId": "4eef9cbd-0e16-45e3-fda6-4d5078012242"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder list\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder 1sLRDYhYwlE9uNs5beDdNyrO80oBs3nzI __pycache__\n",
            "Processing file 14ST9RPnaFI-rgaHgmwQrG7LDEy3Z-OPs abstract_agent.py\n",
            "Processing file 1uOowZtnqB-Df3n5nhoSCoXNPBod40ZKk atari_helpers.py\n",
            "Processing file 1xi6hZIOR_R_eQO9het6sy2m7h2GRheu2 check_test.py\n",
            "Processing file 1Lw5_R_Y0Gk1nNSeG264I9M1w1N1WCyMF loggers.py\n",
            "Processing file 1HQHDLpU7p3YI1Av_jFunScokmy10jgTX plot_utils.py\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14ST9RPnaFI-rgaHgmwQrG7LDEy3Z-OPs\n",
            "To: /content/external/abstract_agent.py\n",
            "100%|██████████| 835/835 [00:00<00:00, 2.08MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uOowZtnqB-Df3n5nhoSCoXNPBod40ZKk\n",
            "To: /content/external/atari_helpers.py\n",
            "100%|██████████| 9.11k/9.11k [00:00<00:00, 16.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xi6hZIOR_R_eQO9het6sy2m7h2GRheu2\n",
            "To: /content/external/check_test.py\n",
            "100%|██████████| 1.56k/1.56k [00:00<00:00, 3.50MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Lw5_R_Y0Gk1nNSeG264I9M1w1N1WCyMF\n",
            "To: /content/external/loggers.py\n",
            "100%|██████████| 982/982 [00:00<00:00, 2.14MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HQHDLpU7p3YI1Av_jFunScokmy10jgTX\n",
            "To: /content/external/plot_utils.py\n",
            "100%|██████████| 2.28k/2.28k [00:00<00:00, 4.75MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/content/external/abstract_agent.py',\n",
              " '/content/external/atari_helpers.py',\n",
              " '/content/external/check_test.py',\n",
              " '/content/external/loggers.py',\n",
              " '/content/external/plot_utils.py']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    %pip install -q --upgrade gdown\n",
        "    from gdown import download_folder\n",
        "\n",
        "    download_folder(\"https://drive.google.com/drive/folders/1SW56nbccfHJtC6oGBIcp7XCeJDkKehGK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhpjsB9bZE-8"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTdn0YfeZE-8",
        "outputId": "b80af935-b178-4b57-9b3a-605e41c16a7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import deque\n",
        "from contextlib import suppress\n",
        "from datetime import datetime\n",
        "from random import sample\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Model\n",
        "from keras.layers import Conv2D, Dense, Flatten, Input, Lambda, multiply\n",
        "from keras.losses import huber_loss\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "from pyvirtualdisplay import Display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# local files\n",
        "from external.abstract_agent import AbstractAgent\n",
        "from external.atari_helpers import LazyFrames, make_atari, wrap_deepmind\n",
        "from external.loggers import TensorBoardLogger, tf_summary_image\n",
        "from external.plot_utils import plot_statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OXBi9qXufV-l"
      },
      "outputs": [],
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "is_ipython = 'inline' in plt.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "    from IPython.display import SVG\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgU4OVMIZE-9"
      },
      "source": [
        "### Extended DQN-Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iaJZTcGhZE-9"
      },
      "outputs": [],
      "source": [
        "class AbstractDQNAgent(AbstractAgent):\n",
        "    __slots__ = [\n",
        "        \"action_size\",\n",
        "        \"state_size\",\n",
        "        \"gamma\",\n",
        "        \"epsilon\",\n",
        "        \"epsilon_decay\",\n",
        "        \"epsilon_min\",\n",
        "        \"alpha\",\n",
        "        \"batch_size\",\n",
        "        \"memory_size\",\n",
        "        \"start_replay_step\",\n",
        "        \"target_model_update_interval\",\n",
        "        \"train_freq\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self,\n",
        "                 action_size: int,\n",
        "                 state_size: int,\n",
        "                 gamma: float,\n",
        "                 epsilon: float,\n",
        "                 epsilon_decay: float,\n",
        "                 epsilon_min: float,\n",
        "                 alpha: float,\n",
        "                 batch_size: int,\n",
        "                 memory_size: int,\n",
        "                 start_replay_step: int,\n",
        "                 target_model_update_interval: int,\n",
        "                 train_freq: int,\n",
        "                 ):\n",
        "        self.action_size = action_size\n",
        "        self.state_size = state_size\n",
        "\n",
        "        self.replay_has_started = False\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.memory_size = memory_size\n",
        "        self.memory = deque(maxlen=self.memory_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.step = 0\n",
        "        self.start_replay_step = start_replay_step\n",
        "\n",
        "        self.target_model_update_interval = target_model_update_interval\n",
        "\n",
        "        self.train_freq = train_freq\n",
        "\n",
        "        assert self.start_replay_step >= self.batch_size, \"The number of steps to start replay must be at least as large as the batch size\"\n",
        "\n",
        "        self.action_mask = np.ones((1, self.action_size))\n",
        "        self.action_mask_batch = np.ones((self.batch_size, self.action_size))\n",
        "\n",
        "        self.tf_config_intra_threads = 8\n",
        "        self.tf_config_inter_threads = 4\n",
        "        self.tf_config_soft_placement = True\n",
        "        self.tf_config_allow_growth = True\n",
        "\n",
        "        config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=self.tf_config_intra_threads,\n",
        "                                inter_op_parallelism_threads=self.tf_config_inter_threads,\n",
        "                                allow_soft_placement=self.tf_config_soft_placement\n",
        "                                )\n",
        "\n",
        "        config.gpu_options.allow_growth = self.tf_config_allow_growth\n",
        "        session = tf.compat.v1.Session(config=config)\n",
        "        set_session(session)  # set this TensorFlow session as the default session for Keras\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "\n",
        "    def save(self, target_path: str) -> None:\n",
        "      \"\"\"\n",
        "        Saves the current state of the DQNAgent to some output files.\n",
        "        Together with `load` this serves as a very rudimentary checkpointing.\n",
        "      \"\"\"\n",
        "      agent_dict = {\n",
        "            \"agent_init\": {},\n",
        "            \"agent_params\": {},\n",
        "            \"tf_config\": {}\n",
        "        }\n",
        "\n",
        "      if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "\n",
        "      for slot in self.__slots__:\n",
        "          agent_dict[\"agent_init\"].update({slot: getattr(self, slot)})\n",
        "\n",
        "      agent_dict[\"agent_init\"].update({\"memory_size\": self.memory.maxlen})\n",
        "\n",
        "      for attr in [\"action_mask\", \"action_mask_batch\"]:\n",
        "          agent_dict[\"agent_params\"].update({attr: getattr(self, attr).tolist()})\n",
        "\n",
        "      agent_dict[\"agent_params\"].update({\"memory\": list(self.memory)})\n",
        "\n",
        "      for tf_config in [\n",
        "          \"tf_config_intra_threads\",\n",
        "          \"tf_config_inter_threads\",\n",
        "          \"tf_config_soft_placement\",\n",
        "          \"tf_config_allow_growth\",\n",
        "      ]:\n",
        "          agent_dict[\"tf_config\"].update({tf_config: getattr(self, tf_config)})\n",
        "\n",
        "      with open(os.path.join(target_path, \"agent.json\"), \"w\") as f:\n",
        "          json.dump(agent_dict, f)\n",
        "\n",
        "      self.model.save_weights(os.path.join(target_path, \"model.h5\"))\n",
        "      self.target_model.save_weights(os.path.join(target_path, \"target_model.h5\"))\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> \"AbstractDQNAgent\":\n",
        "      \"\"\"\n",
        "        Loads the serialized state of a DQNAgent and returns an instance of it.\n",
        "      \"\"\"\n",
        "\n",
        "      with open(os.path.join(path, \"agent.json\"), \"r\") as f:\n",
        "          agent_dict = json.load(f)\n",
        "\n",
        "      agent = cls(**agent_dict[\"agent_init\"])\n",
        "\n",
        "      agent.action_mask = np.array(agent_dict[\"agent_params\"][\"action_mask\"])\n",
        "      agent.action_mask_batch = np.array(agent_dict[\"agent_params\"][\"action_mask_batch\"])\n",
        "\n",
        "      config = tf.compat.v1.ConfigProto(\n",
        "          intra_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_intra_threads\"],\n",
        "          inter_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_inter_threads\"],\n",
        "          allow_soft_placement=agent_dict[\"tf_config\"][\"tf_config_soft_placement\"])\n",
        "\n",
        "      config.gpu_options.allow_growth = agent_dict[\"tf_config\"][\"tf_config_allow_growth\"]\n",
        "      session = tf.compat.v1.Session(config=config)\n",
        "      set_session(session)\n",
        "\n",
        "      agent.model.load_weights('model.h5')\n",
        "      agent.target_model.load_weights(\"target_model.h5\")\n",
        "\n",
        "      return agent\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, experience):\n",
        "      raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def act(self, state):\n",
        "      raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def _build_model(self) -> Model:\n",
        "      raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efDYrUf9ZE--"
      },
      "source": [
        "## Deep Q-Learning Network (DQN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qADJ-_MoZE--",
        "outputId": "1bd87456-97fa-4795-fa7b-0dd1272ad3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NoopResetEnv (max 30) wrapper is used.\n",
            "MaxAndSkipEnv (skip 4) wrapper is used.\n",
            "EpisodicLifeEnv wrapper is used.\n",
            "FireResetEnv wrapper is used.\n",
            "ClipRewardEnv wrapper is used.\n",
            "FrameStack (4) wrapper is used.\n"
          ]
        }
      ],
      "source": [
        "env = make_atari(\"VideoPinball-v4\")\n",
        "env = wrap_deepmind(env, frame_stack=True) # maps frames to 84x84x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5v-6xVSZE--"
      },
      "source": [
        "### Create the DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO2DzOVAZE-_"
      },
      "source": [
        "Take the given `AbstractDQNAgent` (previously called `DQNAgent`) and add missing methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I9nQIr7xZE-_"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(AbstractDQNAgent):\n",
        "    def _build_model(self) -> Model:\n",
        "        \"\"\"Deep Q-network as defined in the DeepMind article on Nature\n",
        "        \n",
        "        Returns:\n",
        "            Model: Tensorflow Model which will be used as internal deep neural network\n",
        "        \"\"\"\n",
        "\n",
        "        atari_shape = (84, 84, 4)\n",
        "\n",
        "        # Frames from the observation\n",
        "        frames_input = Input(atari_shape, name=\"frames\")\n",
        "\n",
        "        # Actions as input\n",
        "        action_mask = Input((self.action_size,), name=\"action_mask\")\n",
        "\n",
        "        # Normalize the frames from [0, 255] to [0, 1]\n",
        "        normalized = Lambda(lambda x: x / 255.0, name=\"normalization\")(frames_input)\n",
        "\n",
        "        # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the \n",
        "        # input image and applies a rectifier nonlinearity.\"\n",
        "        # Results in an output shape of (20, 20, 16)\n",
        "        conv1 = Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(8, 8),\n",
        "            strides=(4, 4),\n",
        "            activation=\"relu\"\n",
        "        )(normalized)\n",
        "\n",
        "        # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed \n",
        "        # by a rectifier nonlinearity.\" \n",
        "        # Results in an output shape of (9, 9, 32)\n",
        "        conv2 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(4,4),\n",
        "            strides=(2,2),\n",
        "            activation=\"relu\"\n",
        "        )(conv1)\n",
        "\n",
        "        conv3 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(4,4),\n",
        "            strides=(1,1),\n",
        "            activation=\"relu\"\n",
        "        )(conv2)\n",
        "\n",
        "        # Flattening the last convolutional layer.\n",
        "        conv_flattened = Flatten()(conv3)\n",
        "\n",
        "        # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
        "        hidden = Dense(units=512, activation='relu')(conv_flattened)\n",
        "\n",
        "        # \"The output layer is a fully-connected linear layer with a single output \n",
        "        # for each valid action.\"\n",
        "        output = Dense(self.action_size)(hidden)\n",
        "\n",
        "        # Multiply the output with the action mask to get only one action output\n",
        "        filtered_output = multiply([output, action_mask])\n",
        "\n",
        "        model = Model(inputs=[frames_input, action_mask], outputs=filtered_output)\n",
        "        model.compile(loss=huber_loss, optimizer=Adam(learning_rate=self.alpha), metrics=None)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def act(self, state: LazyFrames) -> int:\n",
        "        \"\"\"Selects the action to be executed based on the given state.\n",
        "\n",
        "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
        "        epsilon, a random action is selected.\n",
        "\n",
        "        Args:\n",
        "            state [LazyFrames]: LazyFrames object representing the state based on 4 stacked observations (images)\n",
        "\n",
        "        Returns:\n",
        "            action [int]\n",
        "        \"\"\"\n",
        "\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # ! TODO self.model.predict oder self.target_model.predict?\n",
        "            q_values = self.model.predict([[np.array(state)], self.action_mask])\n",
        "            action = np.argmax(q_values)\n",
        "        return action\n",
        "\n",
        "        \n",
        "    def train(self, experience: Tuple[LazyFrames, int, LazyFrames, float, bool]):\n",
        "        \"\"\"Stores the experience in memory. If memory is full trains network by replay.\n",
        "\n",
        "        Args:\n",
        "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        \n",
        "        self.memory.append(experience)\n",
        "        \n",
        "        #  - Update epsilon as long as it is not minimal\n",
        "        #  - Update weights of the target model (syn of the two models)\n",
        "        #  - Execute replay\n",
        "        if self.step >= self.start_replay_step:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon -= self.epsilon_decay\n",
        "            if self.step % self.target_model_update_interval == 0:\n",
        "                self.target_model.set_weights(self.model.get_weights())\n",
        "            if self.step % self.train_freq == 0:\n",
        "                self._replay()\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "\n",
        "    def _replay(self) -> None:\n",
        "        \"\"\"Gets random experiences from memory for batch update of Q-function.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        states, actions, next_states, rewards, dones = [np.array(memory) for memory in zip(*sample(self.memory, self.batch_size))]\n",
        "\n",
        "        # ! Can be left out if useless\n",
        "        assert all(isinstance(x, np.ndarray) for x in (states, actions, rewards, next_states, dones)), \\\n",
        "            \"All experience batches should be of type np.ndarray.\"\n",
        "        assert states.shape == (self.batch_size, 84, 84, 4), \\\n",
        "            f\"States shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
        "        assert actions.shape == (self.batch_size,), f\"Actions shape should be: {(self.batch_size,)}\"\n",
        "        assert rewards.shape == (self.batch_size,), f\"Rewards shape should be: {(self.batch_size,)}\"\n",
        "        assert next_states.shape == (self.batch_size, 84, 84, 4), \\\n",
        "            f\"Next states shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
        "        assert dones.shape == (self.batch_size,), f\"Dones shape should be: {(self.batch_size,)}\"\n",
        "\n",
        "        # Predict the Q values of the next states. Passing ones as the action mask.\n",
        "        next_q_values = self.target_model.predict([next_states, self.action_mask_batch], verbose=0)\n",
        "\n",
        "        # Calculate the Q values.\n",
        "        # - Terminal states get the reward\n",
        "        # - Non-terminal states get reward + gamma * max next_state q_value\n",
        "        q_values = [reward + (1 - done) * self.gamma * np.max(next_q_value) for done, reward, next_q_value in zip(dones, rewards, next_q_values)]\n",
        "\n",
        "        # Create a one hot encoding of the actions (the selected action is 1 all others 0)\n",
        "        one_hot_actions = to_categorical(actions, num_classes=self.action_size)\n",
        "\n",
        "        # Create the target Q values based on the one hot encoding of the actions and the calculated Q values\n",
        "        # This can be seen as matrix multiplication\n",
        "        # q_values = [0.5, 0.7, 0.9]\n",
        "        # actions [[1. 0. 0. 0.]\n",
        "        #          [0. 0. 1. 0.]\n",
        "        #          [0. 0. 0. 1.]]\n",
        "        # output  [[0.5 0.  0.   0. ]\n",
        "        #          [0.  0.  0.7  0. ]\n",
        "        #          [0.  0.  0.9  0. ]]\n",
        "        target_q_values = np.array(q_values)[np.newaxis].T * one_hot_actions\n",
        "\n",
        "        # Fit the model with the given states and the selected actions as one hot vector and the target_q_values as y\n",
        "        self.model.fit(\n",
        "           x=[states, one_hot_actions],  # states and mask\n",
        "           y=target_q_values,  # target Q values\n",
        "           batch_size=self.batch_size,\n",
        "           verbose=0\n",
        "        )\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nkMxNKmAZE_A"
      },
      "outputs": [],
      "source": [
        "def interact_with_environment(env, agent, n_episodes=600, max_steps=1000000, train=True, verbose=True):      \n",
        "    statistics = []\n",
        "    tb_logger = TensorBoardLogger(f'./logs/run-{datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}')\n",
        "    \n",
        "    with suppress(KeyboardInterrupt):\n",
        "        total_step = 0\n",
        "        for episode in range(n_episodes):\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            state = env.reset()\n",
        "            episode_start_time = time.time()\n",
        "            episode_step = 0\n",
        "\n",
        "            while not done:\n",
        "                action = agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if train:\n",
        "                    agent.train((state, action, next_state, reward, done))\n",
        "\n",
        "                if episode == 0:\n",
        "                    # for debug purpose log every state of first episode\n",
        "                    for obs in state:\n",
        "                        tb_logger.log_image(f'state_t{episode_step}:', tf_summary_image(np.array(obs, copy=False)),\n",
        "                                            global_step=total_step)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                episode_step += 1\n",
        "            \n",
        "            total_step += episode_step\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                speed = episode_step / (time.time() - episode_start_time)\n",
        "                tb_logger.log_scalar('score', episode_reward, global_step=total_step)\n",
        "                tb_logger.log_scalar('epsilon', agent.epsilon, global_step=total_step)\n",
        "                tb_logger.log_scalar('speed', speed, global_step=total_step)\n",
        "                if verbose:\n",
        "                    print(f'episode: {episode}/{n_episodes}, score: {episode_reward}, steps: {episode_step}, '\n",
        "                          f'total steps: {total_step}, e: {agent.epsilon:.3f}, speed: {speed:.2f} steps/s')\n",
        "\n",
        "            statistics.append({\n",
        "                'episode': episode,\n",
        "                'score': episode_reward,\n",
        "                'steps': episode_step\n",
        "            })\n",
        "                                  \n",
        "            if total_step >= max_steps:\n",
        "                break\n",
        "        \n",
        "    return statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae5vkyyHZE_A",
        "outputId": "165e0373-e37a-4b60-dcc8-678d83f9b16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/5000, score: 0.0, steps: 36, total steps: 36, e: 1.000, speed: 85.64 steps/s\n",
            "episode: 10/5000, score: 8.0, steps: 112, total steps: 1293, e: 1.000, speed: 99.37 steps/s\n",
            "episode: 20/5000, score: 9.0, steps: 91, total steps: 2979, e: 1.000, speed: 296.66 steps/s\n",
            "episode: 30/5000, score: 1.0, steps: 35, total steps: 4600, e: 1.000, speed: 312.94 steps/s\n",
            "episode: 40/5000, score: 19.0, steps: 111, total steps: 6078, e: 1.000, speed: 314.61 steps/s\n",
            "episode: 50/5000, score: 3.0, steps: 67, total steps: 7470, e: 1.000, speed: 292.77 steps/s\n",
            "episode: 60/5000, score: 70.0, steps: 414, total steps: 8821, e: 1.000, speed: 165.60 steps/s\n",
            "episode: 70/5000, score: 2.0, steps: 85, total steps: 10641, e: 1.000, speed: 297.34 steps/s\n"
          ]
        }
      ],
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "\n",
        "# Hyperparams (should be sufficient)\n",
        "episodes = 5000\n",
        "annealing_steps = 100000  # not episodes!\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.000004\n",
        "alpha = 0.0001\n",
        "batch_size = 64\n",
        "memory_size = 200000\n",
        "start_replay_step = 200000\n",
        "target_model_update_interval = 1000\n",
        "train_freq = 4\n",
        "\n",
        "agent = DQNAgent(action_size=action_size, state_size=state_size, gamma=gamma, \n",
        "                 epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min, \n",
        "                 alpha=alpha, batch_size=batch_size, memory_size=memory_size,\n",
        "                 start_replay_step=start_replay_step, \n",
        "                 target_model_update_interval=target_model_update_interval, train_freq=train_freq)\n",
        "\n",
        "statistics = interact_with_environment(env, agent, n_episodes=episodes, verbose=True)\n",
        "env.close()\n",
        "plot_statistics(statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxXdW7Biea4M"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    state = env.reset()\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    for j in range(200):\n",
        "        action = agent.act(state)\n",
        "        img.set_data(env.render(mode='rgb_array')) \n",
        "        plt.axis('off')\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            break \n",
        "            \n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHZxhRSHf6yU"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(agent.model, to_file='keras_plot_model_2.png', show_shapes=True)\n",
        "display.Image('keras_plot_model_2.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPrMm9odgFs7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from os import path\n",
        "\n",
        "save_dir = \"./saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1d6Ah1GgJ-G"
      },
      "outputs": [],
      "source": [
        "agent.model.save(path.join(save_dir, \"model.tf\"))\n",
        "agent.target_model.save(path.join(save_dir, \"target_model.tf\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjzHcEOHgaun"
      },
      "outputs": [],
      "source": [
        "agent.model = None\n",
        "agent.target_model = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU6_sPNKgb7b"
      },
      "outputs": [],
      "source": [
        "with open(path.join(save_dir, \"agent.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(agent, f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-IOIKZghSn"
      },
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxBJ3gv1ggqA"
      },
      "outputs": [],
      "source": [
        "with open(path.join(save_dir, \"agent.pkl\"), \"rb\") as f:\n",
        "    agent = pickle.load(f)\n",
        "    f.close()\n",
        "\n",
        "agent.model = tf.keras.models.load_model(path.join(save_dir, \"model.tf\"))\n",
        "agent.target_model = tf.keras.models.load_model(path.join(save_dir, \"target_model.tf\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMBpIA_oZE_A"
      },
      "source": [
        "## To-Dos\n",
        "\n",
        "- Create on place for hyperparameters for inside the model and pass it on\n",
        "  - e. g. optimizer, different metrics, checkpointing for the internal model, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5UV7EPfZE_B"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "119d84cb8c3b84b6d48a93e2933b395fcb363c6d1cda3c0de52c91f1ece8e0fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
